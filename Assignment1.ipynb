{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team composed by:\n",
    "- Agostara Nicolò\n",
    "- Fratti Giorgio\n",
    "- Fusillo Antonio\n",
    "- Protti Edoardo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### THEORETICAL FRAMEWORK\n",
    "\n",
    "The dataset contains ratings of items by users\n",
    "\n",
    "For each couple $(u, i)$ of a user and a item, we consider\n",
    "the discrete random variable $R(u, i)$ = \"item i is relevant for user u\"\n",
    "$R(u, i) \\in \\{0, 1\\}$ for every $u, i$\n",
    "In this scenario \"relevant\" doesn't assume the usual meaning\n",
    "it refers to a \"relevance\" by a reccomender system point of view\n",
    "\n",
    "If a user u rated an item i, then he interacted with it,\n",
    "in this case we assume to have a realization $r(u, i) = 1 \\text{ of } R(u, i)$\n",
    "\n",
    "Since we only have items ratings in our dataset, we don't have any \"negative\"\n",
    "realization of $R(u, i)$, i.e. any realization $r(u, i) = 0$\n",
    "\n",
    "For this reason we make a sampling from the unrealized variables, i.e\n",
    "the variables $R(u, i)$ such in our dataset there is not a rating of item i by user u\n",
    "\n",
    "Given our final dataset, consisting in realizations of a subset of the variables $\\{R(u, i): \\text{u user, i item}\\}$\n",
    "we want to maximize the likelihood function of the model, i.e.\n",
    "$$L = \\prod p(R(u, i) = r(u, i)) = (\\prod_{i+} p(R(u, i) = 1)) \\times (\\prod_{i-} p(R(u, i) = 0)$$ \n",
    "splitting the products between positive and negative realizations\n",
    "\n",
    "Our model returns the value $f(u, i) = p(R(u, i) = 1)$ i.e. the \"probability of relevance of item i for user u\"\n",
    "Passing to maximize the log likelihood function we want to maximize:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\log L &= \\sum_{u,i} \\log p(R(u, i) = 1) + \\sum_{u,i} \\log p(R(u,i) = 0)\\\\\n",
    "&= \\sum_{u,i} \\log p(R(u, i) = 1) + \\sum_{u,i} \\log (1 - p(R(u, i) = 1))\\\\\n",
    "&= \\sum_{u,i} \\log f(u, i) + \\sum_{u,i} \\log (1 - f(u, i))\\\\\n",
    "&= ... =\\mathrm{BCE}(f)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "from neumf import NeuMF"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary Definitions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PATH definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"./ml-100k/u.data\"\n",
    "MODEL_PATH = \"./models/\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"seed\": 42,\n",
    "    \"lr\": 0.001,\n",
    "    \"dropout\": 0.2,\n",
    "    \"batch_size\": 256,\n",
    "    \"epochs\": 30,\n",
    "    \"top_k\": 10,\n",
    "    \"num_factors\": 32,\n",
    "    \"layers\": (32, 16, 8),\n",
    "    \"out\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(args[\"seed\"])\n",
    "torch.manual_seed(args[\"seed\"])\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hit(ng_item, pred_items):\n",
    "    return 1 if ng_item in pred_items else 0\n",
    "\n",
    "\n",
    "def ndcg(ng_item, pred_items):\n",
    "    if ng_item in pred_items:\n",
    "        index = pred_items.index(ng_item)\n",
    "        return np.reciprocal(np.log2(index + 2))\n",
    "    return 0\n",
    "\n",
    "\n",
    "def metrics(model, test_loader, top_k, device):\n",
    "    HR, NDCG = [], []\n",
    "\n",
    "    for user, item, label in test_loader:\n",
    "        user = user.to(device)\n",
    "        item = item.to(device)\n",
    "\n",
    "        predictions = model(user, item)\n",
    "        _, indices = torch.topk(predictions, top_k)\n",
    "        recommends = torch.take(item, indices).cpu().numpy().tolist()\n",
    "\n",
    "        ng_item = item[0].item()  # leave one-out evaluation has only one item per user\n",
    "        HR.append(hit(ng_item, recommends))\n",
    "        NDCG.append(ndcg(ng_item, recommends))\n",
    "\n",
    "    return np.mean(HR), np.mean(NDCG)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torch Dataset classes definitions\n",
    "- One class for the Training set\n",
    "- One class for the Validation Set\n",
    "- One class for the Union between Train and Validation Set (we have not trained the model on this Dataset)\n",
    "- One class for the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train dataset is {train_positive} Union {train_negative}\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, table):\n",
    "        self.interactions = []\n",
    "        \n",
    "        for _, user_id, _, train_positive, _, train_negative, _, _ in table.itertuples():\n",
    "\n",
    "            # Positive interactions\n",
    "            for item in train_positive:\n",
    "                self.interactions.append((user_id, item, 1.0))\n",
    "\n",
    "            # Negative interactions\n",
    "            for item in train_negative:\n",
    "                self.interactions.append((user_id, item, 0.0))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.interactions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        user, item, rel = self.interactions[idx]\n",
    "        user_id = torch.tensor(user, dtype = torch.long)\n",
    "        item_id = torch.tensor(item, dtype = torch.long)\n",
    "        relevance = torch.tensor(rel, dtype = torch.float)\n",
    "        return user_id, item_id, relevance\n",
    "\n",
    "\n",
    "# Validation dataset is {valid_positive} Union {valid_negative}\n",
    "class ValidationDataset(Dataset):\n",
    "    def __init__(self, table):\n",
    "        self.interactions = []\n",
    "        \n",
    "        # For each user\n",
    "        for _, user_id, _, _, _, _, valid_positive, valid_negative in table.itertuples():\n",
    "\n",
    "            # Positive interactions\n",
    "            for item in valid_positive:\n",
    "                self.interactions.append((user_id, item, 1.0))\n",
    "\n",
    "            # Negative interactions\n",
    "            for item in valid_negative:\n",
    "                self.interactions.append((user_id, item, 0.0))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.interactions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        user, item, rel = self.interactions[idx]\n",
    "        user_id = torch.tensor(user, dtype = torch.long)\n",
    "        item_id = torch.tensor(item, dtype = torch.long)\n",
    "        relevance = torch.tensor(rel, dtype = torch.float)\n",
    "        return user_id, item_id, relevance\n",
    "\n",
    "\n",
    "# Complete train dataset is {train_positive} Union {train_negative} Union {valid_positive} Union {valid_negative}\n",
    "class CompleteTrainDataset(Dataset):\n",
    "    def __init__(self, train: TrainDataset, valid: ValidationDataset):\n",
    "        self.interactions = train.interactions + valid.interactions\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.interactions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        user, item, rel = self.interactions[idx]\n",
    "        user_id = torch.tensor(user, dtype = torch.long)\n",
    "        item_id = torch.tensor(item, dtype = torch.long)\n",
    "        relevance = torch.tensor(rel, dtype = torch.float)\n",
    "        return user_id, item_id, relevance\n",
    "\n",
    "\n",
    "# Test dataset is {test_positive} Union {100 random samples from unknown}\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, table, num_unknown=100):\n",
    "        self.interactions = []\n",
    "        \n",
    "        for _, user_id, test_positive, _, unknown, _, _, _ in table.itertuples():\n",
    "\n",
    "            # Positive interactions\n",
    "            for item in test_positive:\n",
    "                self.interactions.append((user_id, item, 1.0))\n",
    "\n",
    "            # Unknown interactions\n",
    "            for item in random.sample(list(unknown), num_unknown):\n",
    "                self.interactions.append((user_id, item, np.nan))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.interactions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        user, item, rel = self.interactions[idx]\n",
    "        user_id = torch.tensor(user, dtype = torch.long)\n",
    "        item_id = torch.tensor(item, dtype = torch.long)\n",
    "        relevance = torch.tensor(rel, dtype = torch.float)\n",
    "        return user_id, item_id, relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_users_items(path):\n",
    "    ratings = pd.read_csv(\n",
    "        path,\n",
    "        sep=\"\\t\",\n",
    "        names=[\"user_id\", \"item_id\", \"rating\", \"timestamp\"],\n",
    "    )\n",
    "    return (ratings[\"user_id\"].nunique(), ratings[\"item_id\"].nunique())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comment on how we build the dataset\n",
    "We followed a different approach than the one used in classroom. Since there are a lot of users with more than 200 ratings, sampling their negative interactions as 4 times the number of positive would have produced a lot of redundance in the data. So we actually sample a variable number of negative interactions for each user, depending on how many items he has interacted with: the more the positive interactions, the less negative interactions we sample.\n",
    "The resulting dataset has a ratio of positive interactions to negative interactions of about 1:1\n",
    "This means also that the metrics we use will have different scale comparison w.r.t. the results obtained in classroom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loaders_from_data(path, batch_size):\n",
    "    ratings = pd.read_csv(path,\n",
    "                        sep = '\\t',\n",
    "                        names = ['user_id', 'item_id', 'rating', 'timestamp'],\n",
    "                        )\n",
    "    \n",
    "    num_users = ratings['user_id'].nunique()\n",
    "    num_items = ratings['item_id'].nunique()\n",
    "\n",
    "    known_positive_size_list = list(ratings.groupby('user_id').size())\n",
    "\n",
    "    # I want user indexing to start from 0\n",
    "    ratings['user_id'] = ratings['user_id'] - 1\n",
    "\n",
    "    # I want item indexing to start from 0\n",
    "    ratings['item_id'] = ratings['item_id'] - 1\n",
    "\n",
    "    # For each user consider the set of items of which we know he interacted with\n",
    "    relevance_table = ratings.groupby('user_id')['item_id'].apply(set).reset_index().rename(columns = {'item_id': 'known_positive'})\n",
    "\n",
    "    # Set of all items\n",
    "    items_set = set(range(num_items))\n",
    "\n",
    "    # For each user consider the set of items of which we have NO interaction information\n",
    "    relevance_table['unknown'] = relevance_table['known_positive'].apply(lambda x: items_set - x)\n",
    "\n",
    "    # We don't know any negative interaction\n",
    "    # We create them by sampling:\n",
    "\n",
    "    # For each user\n",
    "    #     Random sample items for each known_positive\n",
    "    #     Assume that the probability of interaction for those items with the user is 0.0\n",
    "\n",
    "    # How many negative interactions to add for each user\n",
    "    def num_ng(user_id):\n",
    "        known_positive_size = known_positive_size_list[user_id]\n",
    "        if known_positive_size <= 50:\n",
    "            return known_positive_size * 4\n",
    "        if known_positive_size <= 100:\n",
    "            return known_positive_size * 2\n",
    "        if known_positive_size <= 200:\n",
    "            return known_positive_size\n",
    "        return 10\n",
    "\n",
    "    relevance_table['known_negative'] = relevance_table.apply(lambda row: set(random.sample(list(row['unknown']), num_ng(row['user_id']))), axis = 'columns')\n",
    "    relevance_table['unknown']  = relevance_table['unknown'] - relevance_table['known_negative']\n",
    "\n",
    "    # Put aside the last known positive interactions for testing models(i.e. leave-one-out evaluation method)\n",
    "    # How to do it:\n",
    "\n",
    "    #     Add column ranking how old is the interaction\n",
    "    ratings['oldness'] = ratings['timestamp'].groupby(ratings['user_id']).rank(method = 'first', ascending = False)\n",
    "\n",
    "    #     Get the most recent interaction for each user\n",
    "    most_recent_interactions = ratings.groupby('oldness').get_group(1.0)\n",
    "\n",
    "    #     Create column to add to relevance_table\n",
    "    last_known_positive = most_recent_interactions.groupby('user_id')['item_id'].apply(set).reset_index().rename(columns = {'item_id': 'last_known_positive'})\n",
    "\n",
    "    #     Add the column\n",
    "    relevance_table = pd.merge(last_known_positive, relevance_table, on = 'user_id')\n",
    "    relevance_table['known_positive'] = relevance_table['known_positive'] - relevance_table['last_known_positive']\n",
    "\n",
    "    # Rename some columns\n",
    "    relevance_table.rename(columns = {'last_known_positive' : 'test_positive', 'known_positive': 'train_positive', 'known_negative': 'train_negative'}, inplace='True')\n",
    "\n",
    "    # Validation data\n",
    "    relevance_table['validation'] = relevance_table.apply(lambda row: set(random.sample(list(row['train_positive'] | row['train_negative']), 1)), axis = 'columns')\n",
    "\n",
    "    relevance_table['valid_positive'] = relevance_table.apply(lambda row: row['validation'] & row['train_positive'], axis = 'columns')\n",
    "    relevance_table['valid_negative'] = relevance_table.apply(lambda row: row['validation'] & row['train_negative'], axis = 'columns')\n",
    "\n",
    "    relevance_table['train_positive']  = relevance_table['train_positive'] - relevance_table['validation']\n",
    "    relevance_table['train_negative']  = relevance_table['train_negative'] - relevance_table['validation']\n",
    "\n",
    "    relevance_table.drop(columns = 'validation', inplace = True)\n",
    "\n",
    "    #print(relevance_table)\n",
    "\n",
    "    train_dataset = TrainDataset(relevance_table)\n",
    "    validation_dataset = ValidationDataset(relevance_table)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
    "    valid_loader = DataLoader(validation_dataset, batch_size = batch_size, shuffle = True)\n",
    "    complete_train_loader = DataLoader(CompleteTrainDataset(train_dataset, validation_dataset), batch_size = batch_size, shuffle = True)\n",
    "    test_loader = DataLoader(TestDataset(relevance_table), batch_size = 101, shuffle = False)\n",
    "\n",
    "    return train_loader, valid_loader, complete_train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, valid_loader, complete_train_loader, test_loader = data_loaders_from_data(PATH, batch_size=256)\n",
    "num_users, num_items = get_num_users_items(PATH)\n",
    "train_loader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models Definition\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training function for GMF and MLP models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trains and save the model and returns the lists of train_loss and validation_loss\n",
    "def train_model(model: nn.Module, train_loader: DataLoader, valid_loader: DataLoader):\n",
    "    model = model.to(device)\n",
    "\n",
    "    loss_function = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args[\"lr\"])\n",
    "\n",
    "    # train, evaluation\n",
    "    best_valid_loss = math.inf\n",
    "    train_loss, valid_loss = [], []\n",
    "\n",
    "    for epoch in tqdm(range(args[\"epochs\"])):\n",
    "        model.train()  # Enable dropout (if present).\n",
    "\n",
    "        intermediate_train_loss = []\n",
    "        for user, item, label in train_loader:\n",
    "            user = user.to(device)\n",
    "            item = item.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            prediction = model(user, item)\n",
    "            loss = loss_function(prediction, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            intermediate_train_loss.append(loss.cpu().detach().item())\n",
    "\n",
    "        train_loss.append(np.mean(intermediate_train_loss))\n",
    "        model.eval()\n",
    "\n",
    "        intermediate_valid_loss = []\n",
    "        for user, item, label in valid_loader:\n",
    "            user = user.to(device)\n",
    "            item = item.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            prediction = model(user, item)\n",
    "            loss = loss_function(prediction, label)\n",
    "\n",
    "            intermediate_valid_loss.append(loss.cpu().detach().item())\n",
    "\n",
    "        valid_loss.append(np.mean(intermediate_valid_loss))\n",
    "        if valid_loss[epoch] < best_valid_loss:\n",
    "            best_valid_loss = valid_loss[epoch]\n",
    "            if args[\"out\"]:\n",
    "                if not os.path.exists(MODEL_PATH):\n",
    "                    os.mkdir(MODEL_PATH)\n",
    "                torch.save(\n",
    "                    model,\n",
    "                    f\"{MODEL_PATH}{model.__class__.__name__}{model.num_factors}.pt\",\n",
    "                )\n",
    "\n",
    "    losses = pd.DataFrame({\"train loss\": train_loss, \"valid loss\": valid_loss})\n",
    "    losses.to_csv(f\"{MODEL_PATH}{model.__class__.__name__}{model.num_factors}.csv\")\n",
    "    return train_loss, valid_loss\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Matrix Factorization (GMF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GMF(nn.Module):\n",
    "    def __init__(self, num_users, num_items):\n",
    "        super(GMF, self).__init__()\n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "        self.num_factors = args[\"num_factors\"]\n",
    "\n",
    "        self.embedding_user = nn.Embedding(\n",
    "            num_embeddings=self.num_users, embedding_dim=self.num_factors\n",
    "        )\n",
    "        self.embedding_item = nn.Embedding(\n",
    "            num_embeddings=self.num_items, embedding_dim=self.num_factors\n",
    "        )\n",
    "\n",
    "        self.affine_output = nn.Linear(in_features=self.num_factors, out_features=1)\n",
    "        self.logistic = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, user_indices, item_indices):\n",
    "        user_embedding = self.embedding_user(user_indices)\n",
    "        item_embedding = self.embedding_item(item_indices)\n",
    "        element_product = torch.mul(user_embedding, item_embedding)\n",
    "        logits = self.affine_output(element_product)\n",
    "        rating = self.logistic(logits)\n",
    "        return rating.squeeze()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and save of GMF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [05:01<00:00, 10.05s/it]\n"
     ]
    }
   ],
   "source": [
    "gmf_model = GMF(num_users, num_items)\n",
    "train_loss, valid_loss = train_model(gmf_model, train_loader, valid_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Layer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, num_users, num_items):\n",
    "        super(MLP, self).__init__()\n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "        self.num_factors = args[\"num_factors\"]\n",
    "\n",
    "        self.embedding_user = nn.Embedding(\n",
    "            num_embeddings=num_users, embedding_dim=args[\"num_factors\"]\n",
    "        )\n",
    "        self.embedding_item = nn.Embedding(\n",
    "            num_embeddings=num_items, embedding_dim=args[\"num_factors\"]\n",
    "        )\n",
    "\n",
    "        layer_sizes = args[\"layers\"]\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(args[\"num_factors\"] * 2, layer_sizes[0]))\n",
    "        for in_size, out_size in zip(layer_sizes[:-1], layer_sizes[1:]):\n",
    "            layers.append(nn.Linear(in_size, out_size))\n",
    "            layers.append(nn.ReLU())\n",
    "        self.mlp_fc = nn.Sequential(*layers)\n",
    "        self.mlp_fc.add_module(\"affine\", nn.Linear(layer_sizes[-1], 1))\n",
    "        self.mlp_fc.add_module(\"logit\", nn.Sigmoid())\n",
    "\n",
    "    def forward(self, user_indices, item_indices):\n",
    "        user_embedding = self.embedding_user(user_indices)\n",
    "        item_embedding = self.embedding_item(item_indices)\n",
    "        vector = torch.cat([user_embedding, item_embedding], dim=-1)\n",
    "        rating = self.mlp_fc(vector)\n",
    "        return rating.squeeze()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Save of MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model = MLP(num_users, num_items)\n",
    "train_loss, valid_loss = train_model(mlp_model, train_loader, valid_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 - Neural Matrix Factorization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HyperParameter Tuning With RayTune\n",
    "If there is a need for a more detailed description of how to use and configure the Hyper Parameter Tuning procedure we will provide them in the next version of the notebook."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We first need to define a config dict that defines the search space of the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"lr\": tune.choice([0.005, 0.015, 0.01, 0.001]),\n",
    "    \"batch_size\": tune.choice([128, 256, 512]),\n",
    "    \"num_factors\": tune.choice([8, 16, 32]),\n",
    "    \"epochs\": tune.choice([15, 30, 40]),\n",
    "    \"out\": True,\n",
    "    \"layers\": tune.choice([[32, 16, 8], [64, 32, 8]]),\n",
    "    \"dropout\": np.random.choice([0, 0.2, 0.5], size=(7, 20), p=[0.5, 0.3, 0.2])\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Function\n",
    "We have to define a train function that takes as argument the config file.\n",
    "This function will:\n",
    "- Initialize NeuMF model with the specified config parameters (num factors, layers, dropout vector for the different layers)\n",
    "- Initialize Train and validation dataloaders\n",
    "- Train the model on the Train dataloader\n",
    "- Evaluate the validation loss and report it to RayTune\n",
    "- Save the best model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trains and save the model and returns the lists of train_loss and validation_loss\n",
    "def train_NeuMF(config):\n",
    "    # model = NeuMF_parametric_dropout(config['num_factors'], num_users, num_items, config['layers'], config['dropout'])\n",
    "    model = NeuMF(\n",
    "        config[\"num_factors\"], num_users, num_items, config[\"layers\"], config[\"dropout\"]\n",
    "    )\n",
    "\n",
    "    train_loader, valid_loader, _, _ = data_loaders_from_data(\n",
    "        PATH, batch_size=config[\"batch_size\"]\n",
    "    )\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    loss_function = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
    "\n",
    "    # train, evaluation\n",
    "    best_valid_loss = math.inf\n",
    "    train_loss, valid_loss = [], []\n",
    "\n",
    "    for epoch in range(config[\"epochs\"]):\n",
    "        model.train()  # Enable dropout (if present).\n",
    "\n",
    "        intermediate_train_loss = []\n",
    "        for user, item, label in train_loader:\n",
    "            user = user.to(device)\n",
    "            item = item.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            prediction = model(user, item)\n",
    "            loss = loss_function(prediction, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            intermediate_train_loss.append(loss.cpu().detach().item())\n",
    "\n",
    "        train_loss.append(np.mean(intermediate_train_loss))\n",
    "        model.eval()\n",
    "\n",
    "        intermediate_valid_loss = []\n",
    "        for user, item, label in valid_loader:\n",
    "            user = user.to(device)\n",
    "            item = item.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            prediction = model(user, item)\n",
    "            loss = loss_function(prediction, label)\n",
    "\n",
    "            intermediate_valid_loss.append(loss.cpu().detach().item())\n",
    "\n",
    "        valid_loss.append(np.mean(intermediate_valid_loss))\n",
    "        if valid_loss[epoch] < best_valid_loss:\n",
    "            best_valid_loss = valid_loss[epoch]\n",
    "            if config[\"out\"]:\n",
    "                if not os.path.exists(MODEL_PATH):\n",
    "                    os.mkdir(MODEL_PATH)\n",
    "                torch.save(\n",
    "                    model,\n",
    "                    f\"{MODEL_PATH}{model.__class__.__name__}{model.num_factors}.pt\",\n",
    "                )\n",
    "\n",
    "        with tune.checkpoint_dir(epoch) as checkpoint_dir:\n",
    "            path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "            torch.save((model.state_dict(), optimizer.state_dict()), path)\n",
    "\n",
    "        tune.report(loss=valid_loss[epoch])\n",
    "\n",
    "    losses = pd.DataFrame({\"train loss\": train_loss, \"valid loss\": valid_loss})\n",
    "    losses.to_csv(f\"{MODEL_PATH}{model.__class__.__name__}{model.num_factors}.csv\")\n",
    "    return train_loss, valid_loss\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the scheduler and running the HT procedure\n",
    "There is a paper related to the ASAH Scheduler at :https://arxiv.org/abs/1810.05934"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = ASHAScheduler(\n",
    "        metric=\"loss\",\n",
    "        mode=\"min\",\n",
    "        max_t=40,\n",
    "        grace_period=5,\n",
    "        reduction_factor=2)\n",
    "\n",
    "reporter = CLIReporter(\n",
    "        metric_columns=[\"loss\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = tune.run(\n",
    "    train_NeuMF,\n",
    "    local_dir=\"./checkpoints/\",\n",
    "    resources_per_trial={\"cpu\": 4},\n",
    "    config=config,\n",
    "    num_samples=10,\n",
    "    scheduler=scheduler,\n",
    "    progress_reporter=reporter,\n",
    "    checkpoint_at_end=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n",
    "best_checkpoint_dir = best_trial.checkpoint.dir_or_data\n",
    "model_state, optimizer_state = torch.load(os.path.join(\n",
    "    best_checkpoint_dir, \"checkpoint\"))\n",
    "\n",
    "best_trained_model = NeuMF(best_trial.config[\"num_factors\"], num_users, num_items, best_trial.config[\"layers\"], best_trial.config['dropout']) \n",
    "\n",
    "best_trained_model.load_state_dict(model_state)\n",
    "torch.save(best_trained_model, '{}{}{}_best_so_far.pt'.format(MODEL_PATH, best_trained_model.__class__.__name__, best_trained_model.factor_num))\n",
    "\n",
    "best_trial.config"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reporting the best configuration in our tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_config = {\n",
    "    \"lr\": 0.01,\n",
    "    \"batch_size\": 256,\n",
    "    \"num_factors\": 8,\n",
    "    \"epochs\": 20,\n",
    "    \"out\": True,\n",
    "    \"layers\": (64, 16, 8),\n",
    "    \"dropout\": (0, 0, 0, 0, 0, 0, 0.5),\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 - Models Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(model: nn.Module, train_loss, valid_loss, ax: plt.Axes):\n",
    "    ax.plot(range(1, len(train_loss) + 1), train_loss, label=\"Training\", color=\"blue\")\n",
    "    ax.plot(range(1, len(valid_loss) + 1), valid_loss, label=\"Validation\", color=\"red\")\n",
    "    ax.legend()\n",
    "    ax.set_title(\n",
    "        f\"{model.__class__.__name__} losses with {model.factor_num} num factors\"\n",
    "    )\n",
    "    ax.set_xticks(range(1, len(train_loss) + 1, step=2))\n",
    "    ax.set_ylim(0.0, 0.7)\n",
    "    ax.set_xlabel(\"Epochs\")\n",
    "    ax.set_ylabel(\"BCE Loss\")\n",
    "    return ax\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation set error of the three models\n",
    "\n",
    "We are aware of the fact that the NeuMF validation loss curve is very bad but this model configuration is the one that performs better (among the ones tested) with the Hit Rate metric and The Normalized Discounted Cumulative Gain.\n",
    "\n",
    "A lower learning rate provide a better validation loss curve but lower metrics performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmf_model = torch.load(MODEL_PATH + \"GMF32.pt\", map_location = device)\n",
    "gmf_losses = pd.read_csv(MODEL_PATH + \"GMF32\", index_col = 0)\n",
    "\n",
    "mlp_model = torch.load(MODEL_PATH + \"MLP32.pt\", map_location = device)\n",
    "mlp_losses = pd.read_csv(MODEL_PATH + \"MLP32\", index_col = 0)\n",
    "\n",
    "nmf_model = torch.load(MODEL_PATH + \"NeuMF8.pt\", map_location = device)\n",
    "nmf_losses = pd.read_csv(MODEL_PATH + \"NeuMF8\", index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize = (20,5))\n",
    "ax1 = plot_losses(nmf_model, nmf_losses['train loss'], nmf_losses['valid loss'], ax1)\n",
    "ax2 = plot_losses(gmf_model, gmf_losses['train loss'], gmf_losses['valid loss'], ax2)\n",
    "ax3 = plot_losses(mlp_model, mlp_losses['train loss'], mlp_losses['valid loss'], ax3)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HR and NDCG Metrics evaluation\n",
    "We have evaluated those metrics while the number of recommendations k vary, we have reported that the best models of NeuMF is with 8 num factors so we take into consideration that model, in the next session different models with different num factors will be evaluated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "def model_evaluation(models_dict, test_loader, k_range = range(5, 15), device = torch.device(\"cpu\")): #the function expects a dict {\"model name\": model}\n",
    "  for key in models_dict:\n",
    "    models_dict[key] = models_dict[key].to(device)\n",
    "\n",
    "  HR = np.zeros((len(k_range), len(models_dict)))\n",
    "  NDCG = np.zeros((len(k_range), len(models_dict)))\n",
    "  for k in tqdm(range(0, k_range.stop - k_range.start)):\n",
    "    for model_index, key in enumerate(models_dict):\n",
    "      HR[k, model_index], NDCG[k, model_index] = metrics(models_dict[key], test_loader, k + 1, device)\n",
    "\n",
    "  HR_result = pd.DataFrame(data = HR, columns = models_dict.keys(), index = k_range)\n",
    "  NDCG_result = pd.DataFrame(data = NDCG, columns = models_dict.keys(), index = k_range)\n",
    "  return HR_result, NDCG_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model = torch.load(MODEL_PATH + \"MLP32.pt\", map_location = device)\n",
    "gmf_model = torch.load(MODEL_PATH + \"GMF32.pt\", map_location = device)\n",
    "nmf_model = torch.load(MODEL_PATH + \"NeuMF8.pt\", map_location = device)\n",
    "mlp_model = mlp_model.to(device)\n",
    "gmf_model = gmf_model.to(device)\n",
    "nmf_model = nmf_model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dict = {\"GMF\": gmf_model, \"MLP\": mlp_model, \"NeuMF\": nmf_model}\n",
    "HR, NDCG = model_evaluation(models_dict, test_loader, device = device, k_range= range(7, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, (hr_ax, ndcg_ax) = plt.subplots(1,2, figsize = (20, 5))\n",
    "colors = [\"green\", \"blue\", \"purple\"]\n",
    "for num, i in enumerate(HR.columns):\n",
    "  hr_ax.plot(HR[i], label = i, color = colors[num])\n",
    "  ndcg_ax.plot(NDCG[i], label = i, color = colors[num])\n",
    "  hr_ax.plot([10],[HR[i][10]], marker = \"o\", color = colors[num], label = \"HR@10\")\n",
    "  ndcg_ax.plot([10],[NDCG[i][10]], marker = \"o\", color = colors[num], label = \"NDCG@10\")\n",
    "\n",
    "\n",
    "hr_ax.set_title(\"Hit Rate while number of recommendations vary\")\n",
    "ndcg_ax.set_title(\"NDCG Rate while number of recommendations vary\")\n",
    "hr_ax.set_ylabel(\"HR\")\n",
    "hr_ax.set_xlabel(\"K value\")\n",
    "ndcg_ax.set_ylabel(\"NDCG\")\n",
    "ndcg_ax.set_xlabel(\"K value\")\n",
    "hr_ax.legend()\n",
    "ndcg_ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3 - Metrics Evaluation while num factors vary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_factors = [8, 16, 32, 64]\n",
    "#do not change the order of HR and NDCG matrices\n",
    "model_names = [\"GMF Model\", \"MLP Model\", \"NeuMF model\"]\n",
    "mlp_models, gmf_models, nmf_models = {}, {}, {}\n",
    "\n",
    "#I create a dict containing as the key the num_factor and as a value the relative model \n",
    "for num in num_factors:\n",
    "    mlp_models[num] = torch.load(MODEL_PATH + \"MLP{}.pt\".format(num), map_location = device)\n",
    "    gmf_models[num] = torch.load(MODEL_PATH + \"GMF{}.pt\".format(num), map_location = device)\n",
    "    nmf_models[num] = torch.load(MODEL_PATH + \"NeuMF{}.pt\".format(num), map_location = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HR, NDCG = list([]), list([])\n",
    "\n",
    "#for each num factor and for each model I compute the provided metrics on the test set\n",
    "for num in tqdm(num_factors):\n",
    "    hr_gmf, ndcg_gmf = metrics(gmf_models[num], test_loader, 10, device)\n",
    "    hr_mlp, ndcg_mlp = metrics(mlp_models[num], test_loader, 10, device)\n",
    "    hr_nmf, ndcg_nmf = metrics(nmf_models[num], test_loader, 10, device)\n",
    "    HR.append([hr_gmf, hr_mlp, hr_nmf])\n",
    "    NDCG.append([ndcg_gmf, ndcg_mlp, ndcg_nmf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here I transpose the matrices because I need to have on the same row the results of the metrics for the same model (in order to construct the bar plot)\n",
    "HR = np.array(HR).T\n",
    "NDCG = np.array(NDCG).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.container import BarContainer \n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, layout='constrained', figsize = (10,12))\n",
    "x = np.arange(HR.shape[1])\n",
    "offset = -0.2\n",
    "for i in range(len(HR)):\n",
    "    rects1 = ax1.bar(x + offset, np.round(HR[i], 3), width=0.2, label = model_names[i])\n",
    "    rects2 = ax2.bar(x + offset, np.round(NDCG[i], 3), width=0.2, label = model_names[i])\n",
    "    ax1.bar_label(rects1, padding = 2)\n",
    "    ax2.bar_label(rects2, padding = 2)\n",
    "    offset += 0.2\n",
    "\n",
    "ax1.set_ylim(min(HR.reshape(-1,1)) - 0.05, max(HR.reshape(-1,1)) + 0.05)\n",
    "ax2.set_ylim(min(NDCG.reshape(-1,1)) - 0.05, max(NDCG.reshape(-1,1)) + 0.05)\n",
    "\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_ylabel(\"HR\")\n",
    "ax1.set_xticklabels(num_factors)\n",
    "ax1.set_xlabel(\"num factors\")\n",
    "\n",
    "\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_ylabel(\"NDCG\")\n",
    "ax2.set_xlabel(\"num factors\")\n",
    "ax2.set_xticklabels(num_factors)\n",
    "\n",
    "ax1.set_title(\"Hit Rate\")\n",
    "ax2.set_title(\"Normalized Discounted Cumulative Gain\")\n",
    "\n",
    "ax1.legend()\n",
    "ax2.legend()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
