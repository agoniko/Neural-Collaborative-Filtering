{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team composed by:\n",
    "- Agostara Nicolò\n",
    "- Fratti Giorgio\n",
    "- Fusillo Antonio\n",
    "- Protti Edoardo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### THEORETICAL FRAMEWORK\n",
    "\n",
    "The dataset contains ratings of items by users\n",
    "\n",
    "For each couple $(u, i)$ of a user and a item, we consider\n",
    "the discrete random variable $R(u, i)$ = \"item i is relevant for user u\"\n",
    "$R(u, i) \\in \\{0, 1\\}$ for every $u, i$\n",
    "In this scenario \"relevant\" doesn't assume the usual meaning\n",
    "it refers to a \"relevance\" by a reccomender system point of view\n",
    "\n",
    "If a user u rated an item i, then he interacted with it,\n",
    "in this case we assume to have a realization $r(u, i) = 1 \\text{ of } R(u, i)$\n",
    "\n",
    "Since we only have items ratings in our dataset, we don't have any \"negative\"\n",
    "realization of $R(u, i)$, i.e. any realization $r(u, i) = 0$\n",
    "\n",
    "For this reason we make a sampling from the unrealized variables, i.e\n",
    "the variables $R(u, i)$ such in our dataset there is not a rating of item i by user u\n",
    "\n",
    "Given our final dataset, consisting in realizations of a subset of the variables $\\{R(u, i): \\text{u user, i item}\\}$\n",
    "we want to maximize the likelihood function of the model, i.e.\n",
    "$$L = \\prod p(R(u, i) = r(u, i)) = (\\prod_{i+} p(R(u, i) = 1)) \\times (\\prod_{i-} p(R(u, i) = 0)$$ \n",
    "splitting the products between positive and negative realizations\n",
    "\n",
    "Our model returns the value $f(u, i) = p(R(u, i) = 1)$ i.e. the \"probability of relevance of item i for user u\"\n",
    "Passing to maximize the log likelihood function we want to maximize:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\log L &= \\sum_{u,i} \\log p(R(u, i) = 1) + \\sum_{u,i} \\log p(R(u,i) = 0)\\\\\n",
    "&= \\sum_{u,i} \\log p(R(u, i) = 1) + \\sum_{u,i} \\log (1 - p(R(u, i) = 1))\\\\\n",
    "&= \\sum_{u,i} \\log f(u, i) + \\sum_{u,i} \\log (1 - f(u, i))\\\\\n",
    "&= ... =\\mathrm{BCE}(f)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "from neumf import NeuMF"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary Definitions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PATH definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"./ml-100k/u.data\"\n",
    "MODEL_PATH = \"./models/\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"seed\": 42,\n",
    "    \"lr\": 0.001,\n",
    "    \"dropout\": 0.2,\n",
    "    \"batch_size\": 256,\n",
    "    \"epochs\": 30,\n",
    "    \"top_k\": 10,\n",
    "    \"num_factors\": 32,\n",
    "    \"layers\": (32, 16, 8),\n",
    "    \"out\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(args[\"seed\"])\n",
    "torch.manual_seed(args[\"seed\"])\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hit(ng_item, pred_items):\n",
    "    return 1 if ng_item in pred_items else 0\n",
    "\n",
    "\n",
    "def ndcg(ng_item, pred_items):\n",
    "    if ng_item in pred_items:\n",
    "        index = pred_items.index(ng_item)\n",
    "        return np.reciprocal(np.log2(index + 2))\n",
    "    return 0\n",
    "\n",
    "\n",
    "def metrics(model, test_loader, top_k, device):\n",
    "    HR, NDCG = [], []\n",
    "\n",
    "    for user, item, label in test_loader:\n",
    "        user = user.to(device)\n",
    "        item = item.to(device)\n",
    "\n",
    "        predictions = model(user, item)\n",
    "        _, indices = torch.topk(predictions, top_k)\n",
    "        recommends = torch.take(item, indices).cpu().numpy().tolist()\n",
    "\n",
    "        ng_item = item[0].item()  # leave one-out evaluation has only one item per user\n",
    "        HR.append(hit(ng_item, recommends))\n",
    "        NDCG.append(ndcg(ng_item, recommends))\n",
    "\n",
    "    return np.mean(HR), np.mean(NDCG)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rating_Datset(Dataset):\n",
    "\tdef __init__(self, user_list, item_list, rating_list):\n",
    "\t\tsuper(Rating_Datset, self).__init__()\n",
    "\t\tself.user_list = user_list\n",
    "\t\tself.item_list = item_list\n",
    "\t\tself.rating_list = rating_list\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.user_list)\n",
    "\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\tuser = self.user_list[idx]\n",
    "\t\titem = self.item_list[idx]\n",
    "\t\trating = self.rating_list[idx]\n",
    "\t\t\n",
    "\t\treturn (\n",
    "\t\t\ttorch.tensor(user, dtype=torch.long),\n",
    "\t\t\ttorch.tensor(item, dtype=torch.long),\n",
    "\t\t\ttorch.tensor(rating, dtype=torch.float)\n",
    "\t\t\t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NCF_Data(object):\n",
    "\t\"\"\"\n",
    "\tConstruct Dataset for NCF\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, args, ratings):\n",
    "\t\tself.ratings = ratings\n",
    "\t\tself.num_ng = args.num_ng\n",
    "\t\tself.num_ng_test = args.num_ng_test\n",
    "\t\tself.batch_size = args.batch_size\n",
    "\n",
    "\t\tself.preprocess_ratings = self._reindex(self.ratings)\n",
    "\n",
    "\t\tself.user_pool = set(self.ratings['user_id'].unique())\n",
    "\t\tself.item_pool = set(self.ratings['item_id'].unique())\n",
    "\n",
    "\t\tself.train_ratings, self.test_ratings = self._leave_one_out(self.preprocess_ratings)\n",
    "\t\tself.negatives = self._negative_sampling(self.preprocess_ratings)\n",
    "\n",
    "\t\n",
    "\tdef _reindex(self, ratings):\n",
    "\t\t\"\"\"\n",
    "\t\tProcess dataset to reindex userID and itemID, also set rating as binary feedback\n",
    "\t\t\"\"\"\n",
    "\t\tuser_list = list(ratings['user_id'].drop_duplicates())\n",
    "\t\tuser2id = {w: i for i, w in enumerate(user_list)}\n",
    "\n",
    "\t\titem_list = list(ratings['item_id'].drop_duplicates())\n",
    "\t\titem2id = {w: i for i, w in enumerate(item_list)}\n",
    "\n",
    "\t\tratings['user_id'] = ratings['user_id'].apply(lambda x: user2id[x])\n",
    "\t\tratings['item_id'] = ratings['item_id'].apply(lambda x: item2id[x])\n",
    "\t\tratings['rating'] = ratings['rating'].apply(lambda x: float(x > 0))\n",
    "\t\treturn ratings\n",
    "\n",
    "\tdef _leave_one_out(self, ratings):\n",
    "\t\t\"\"\"\n",
    "\t\tleave-one-out evaluation protocol in paper https://www.comp.nus.edu.sg/~xiangnan/papers/ncf.pdf\n",
    "\t\t\"\"\"\n",
    "\t\tratings['rank_latest'] = ratings.groupby(['user_id'])['timestamp'].rank(method='first', ascending=False)\n",
    "\t\ttest = ratings.loc[ratings['rank_latest'] == 1]\n",
    "\t\ttrain = ratings.loc[ratings['rank_latest'] > 1]\n",
    "\t\tassert train['user_id'].nunique()==test['user_id'].nunique(), 'Not Match Train User with Test User'\n",
    "\t\treturn train[['user_id', 'item_id', 'rating']], test[['user_id', 'item_id', 'rating']]\n",
    "\n",
    "\tdef _negative_sampling(self, ratings):\n",
    "\t\tinteract_status = (\n",
    "\t\t\tratings.groupby('user_id')['item_id']\n",
    "\t\t\t.apply(set)\n",
    "\t\t\t.reset_index()\n",
    "\t\t\t.rename(columns={'item_id': 'interacted_items'}))\n",
    "\t\tinteract_status['negative_items'] = interact_status['interacted_items'].apply(lambda x: self.item_pool - x)\n",
    "\t\tinteract_status['negative_samples'] = interact_status['negative_items'].apply(lambda x: random.sample(x, self.num_ng_test))\n",
    "\t\treturn interact_status[['user_id', 'negative_items', 'negative_samples']]\n",
    "\n",
    "\tdef get_train_instance(self):\n",
    "\t\tusers, items, ratings = [], [], []\n",
    "\t\ttrain_ratings = pd.merge(self.train_ratings, self.negatives[['user_id', 'negative_items']], on='user_id')\n",
    "\t\ttrain_ratings['negatives'] = train_ratings['negative_items'].apply(lambda x: random.sample(x, self.num_ng))\n",
    "\t\tfor row in train_ratings.itertuples():\n",
    "\t\t\tusers.append(int(row.user_id))\n",
    "\t\t\titems.append(int(row.item_id))\n",
    "\t\t\tratings.append(float(row.rating))\n",
    "\t\t\tfor i in range(self.num_ng):\n",
    "\t\t\t\tusers.append(int(row.user_id))\n",
    "\t\t\t\titems.append(int(row.negatives[i]))\n",
    "\t\t\t\tratings.append(float(0))  # negative samples get 0 rating\n",
    "\t\tdataset = Rating_Datset(\n",
    "\t\t\tuser_list=users,\n",
    "\t\t\titem_list=items,\n",
    "\t\t\trating_list=ratings)\n",
    "\t\treturn DataLoader(dataset, batch_size=self.batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "\tdef get_test_instance(self):\n",
    "\t\tusers, items, ratings = [], [], []\n",
    "\t\ttest_ratings = pd.merge(self.test_ratings, self.negatives[['user_id', 'negative_samples']], on='user_id')\n",
    "\t\tfor row in test_ratings.itertuples():\n",
    "\t\t\tusers.append(int(row.user_id))\n",
    "\t\t\titems.append(int(row.item_id))\n",
    "\t\t\tratings.append(float(row.rating))\n",
    "\t\t\tfor i in getattr(row, 'negative_samples'):\n",
    "\t\t\t\tusers.append(int(row.user_id))\n",
    "\t\t\t\titems.append(int(i))\n",
    "\t\t\t\tratings.append(float(0))\n",
    "\t\tdataset = Rating_Datset(\n",
    "\t\t\tuser_list=users,\n",
    "\t\t\titem_list=items,\n",
    "\t\t\trating_list=ratings)\n",
    "\t\treturn DataLoader(dataset, batch_size=self.num_ng_test+1, shuffle=False, num_workers=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models Definition\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training function for GMF and MLP models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model: nn.Module, train_loader: DataLoader, test_loader: DataLoader):\n",
    "    model = model.to(device)\n",
    "\n",
    "    loss_function = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args[\"lr\"])\n",
    "\n",
    "    for epoch in tqdm(range(args[\"epochs\"])):\n",
    "       \tstart_time = time.time()\n",
    "        model.train()  # Enable dropout (if present).\n",
    "\n",
    "        intermediate_train_loss = []\n",
    "        for user, item, label in train_loader:\n",
    "            user = user.to(device)\n",
    "            item = item.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            prediction = model(user, item)\n",
    "            loss = loss_function(prediction, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            intermediate_train_loss.append(loss.cpu().detach().item())\n",
    "\n",
    "        model.eval()\n",
    "        HR, NDCG = metrics(model, test_loader, args.top_k, device)\n",
    "        #writer.add_scalar('Perfomance/HR@10', HR, epoch)\n",
    "        #writer.add_scalar('Perfomance/NDCG@10', NDCG, epoch)\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(\"Epoch {:03d}\".format(epoch) + \" time to train: \" + \n",
    "                time.strftime(\"%H: %M: %S\", time.gmtime(elapsed_time)))\n",
    "        print(\"HR: {:.3f}\\tNDCG: {:.3f}\".format(np.mean(HR), np.mean(NDCG)))\n",
    "\n",
    "        if HR > best_hr:\n",
    "            best_hr, best_ndcg, best_epoch = HR, NDCG, epoch\n",
    "            if args.out:\n",
    "                if not os.path.exists(MODEL_PATH):\n",
    "                    os.mkdir(MODEL_PATH)\n",
    "                torch.save(model, \n",
    "                    '{}{}.pt'.format(MODEL_PATH, model.__class__.__name__))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Matrix Factorization (GMF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GMF(nn.Module):\n",
    "    def __init__(self, num_users, num_items):\n",
    "        super(GMF, self).__init__()\n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "        self.num_factors = args[\"num_factors\"]\n",
    "\n",
    "        self.embedding_user = nn.Embedding(\n",
    "            num_embeddings=self.num_users, embedding_dim=self.num_factors\n",
    "        )\n",
    "        self.embedding_item = nn.Embedding(\n",
    "            num_embeddings=self.num_items, embedding_dim=self.num_factors\n",
    "        )\n",
    "\n",
    "        self.affine_output = nn.Linear(in_features=self.num_factors, out_features=1)\n",
    "        self.logistic = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, user_indices, item_indices):\n",
    "        user_embedding = self.embedding_user(user_indices)\n",
    "        item_embedding = self.embedding_item(item_indices)\n",
    "        element_product = torch.mul(user_embedding, item_embedding)\n",
    "        logits = self.affine_output(element_product)\n",
    "        rating = self.logistic(logits)\n",
    "        return rating.squeeze()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and save of GMF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [05:01<00:00, 10.05s/it]\n"
     ]
    }
   ],
   "source": [
    "gmf_model = GMF(num_users, num_items)\n",
    "train_loss, valid_loss = train_model(gmf_model, train_loader, valid_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Layer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, num_users, num_items):\n",
    "        super(MLP, self).__init__()\n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "        self.num_factors = args[\"num_factors\"]\n",
    "\n",
    "        self.embedding_user = nn.Embedding(\n",
    "            num_embeddings=num_users, embedding_dim=args[\"num_factors\"]\n",
    "        )\n",
    "        self.embedding_item = nn.Embedding(\n",
    "            num_embeddings=num_items, embedding_dim=args[\"num_factors\"]\n",
    "        )\n",
    "\n",
    "        layer_sizes = args[\"layers\"]\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(args[\"num_factors\"] * 2, layer_sizes[0]))\n",
    "        for in_size, out_size in zip(layer_sizes[:-1], layer_sizes[1:]):\n",
    "            layers.append(nn.Linear(in_size, out_size))\n",
    "            layers.append(nn.ReLU())\n",
    "        self.mlp_fc = nn.Sequential(*layers)\n",
    "        self.mlp_fc.add_module(\"affine\", nn.Linear(layer_sizes[-1], 1))\n",
    "        self.mlp_fc.add_module(\"logit\", nn.Sigmoid())\n",
    "\n",
    "    def forward(self, user_indices, item_indices):\n",
    "        user_embedding = self.embedding_user(user_indices)\n",
    "        item_embedding = self.embedding_item(item_indices)\n",
    "        vector = torch.cat([user_embedding, item_embedding], dim=-1)\n",
    "        rating = self.mlp_fc(vector)\n",
    "        return rating.squeeze()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Save of MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model = MLP(num_users, num_items)\n",
    "train_loss, valid_loss = train_model(mlp_model, train_loader, valid_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 - Neural Matrix Factorization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 - Models Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(model: nn.Module, train_loss, valid_loss, ax: plt.Axes):\n",
    "    ax.plot(range(1, len(train_loss) + 1), train_loss, label=\"Training\", color=\"blue\")\n",
    "    ax.plot(range(1, len(valid_loss) + 1), valid_loss, label=\"Validation\", color=\"red\")\n",
    "    ax.legend()\n",
    "    ax.set_title(\n",
    "        f\"{model.__class__.__name__} losses with {model.factor_num} num factors\"\n",
    "    )\n",
    "    ax.set_xticks(range(1, len(train_loss) + 1, step=2))\n",
    "    ax.set_ylim(0.0, 0.7)\n",
    "    ax.set_xlabel(\"Epochs\")\n",
    "    ax.set_ylabel(\"BCE Loss\")\n",
    "    return ax\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation set error of the three models\n",
    "\n",
    "We are aware of the fact that the NeuMF validation loss curve is very bad but this model configuration is the one that performs better (among the ones tested) with the Hit Rate metric and The Normalized Discounted Cumulative Gain.\n",
    "\n",
    "A lower learning rate provide a better validation loss curve but lower metrics performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmf_model = torch.load(MODEL_PATH + \"GMF32.pt\", map_location = device)\n",
    "gmf_losses = pd.read_csv(MODEL_PATH + \"GMF32\", index_col = 0)\n",
    "\n",
    "mlp_model = torch.load(MODEL_PATH + \"MLP32.pt\", map_location = device)\n",
    "mlp_losses = pd.read_csv(MODEL_PATH + \"MLP32\", index_col = 0)\n",
    "\n",
    "nmf_model = torch.load(MODEL_PATH + \"NeuMF8.pt\", map_location = device)\n",
    "nmf_losses = pd.read_csv(MODEL_PATH + \"NeuMF8\", index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize = (20,5))\n",
    "ax1 = plot_losses(nmf_model, nmf_losses['train loss'], nmf_losses['valid loss'], ax1)\n",
    "ax2 = plot_losses(gmf_model, gmf_losses['train loss'], gmf_losses['valid loss'], ax2)\n",
    "ax3 = plot_losses(mlp_model, mlp_losses['train loss'], mlp_losses['valid loss'], ax3)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HR and NDCG Metrics evaluation\n",
    "We have evaluated those metrics while the number of recommendations k vary, we have reported that the best models of NeuMF is with 8 num factors so we take into consideration that model, in the next session different models with different num factors will be evaluated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "def model_evaluation(models_dict, test_loader, k_range = range(5, 15), device = torch.device(\"cpu\")): #the function expects a dict {\"model name\": model}\n",
    "  for key in models_dict:\n",
    "    models_dict[key] = models_dict[key].to(device)\n",
    "\n",
    "  HR = np.zeros((len(k_range), len(models_dict)))\n",
    "  NDCG = np.zeros((len(k_range), len(models_dict)))\n",
    "  for k in tqdm(range(0, k_range.stop - k_range.start)):\n",
    "    for model_index, key in enumerate(models_dict):\n",
    "      HR[k, model_index], NDCG[k, model_index] = metrics(models_dict[key], test_loader, k + 1, device)\n",
    "\n",
    "  HR_result = pd.DataFrame(data = HR, columns = models_dict.keys(), index = k_range)\n",
    "  NDCG_result = pd.DataFrame(data = NDCG, columns = models_dict.keys(), index = k_range)\n",
    "  return HR_result, NDCG_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model = torch.load(MODEL_PATH + \"MLP32.pt\", map_location = device)\n",
    "gmf_model = torch.load(MODEL_PATH + \"GMF32.pt\", map_location = device)\n",
    "nmf_model = torch.load(MODEL_PATH + \"NeuMF8.pt\", map_location = device)\n",
    "mlp_model = mlp_model.to(device)\n",
    "gmf_model = gmf_model.to(device)\n",
    "nmf_model = nmf_model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dict = {\"GMF\": gmf_model, \"MLP\": mlp_model, \"NeuMF\": nmf_model}\n",
    "HR, NDCG = model_evaluation(models_dict, test_loader, device = device, k_range= range(7, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, (hr_ax, ndcg_ax) = plt.subplots(1,2, figsize = (20, 5))\n",
    "colors = [\"green\", \"blue\", \"purple\"]\n",
    "for num, i in enumerate(HR.columns):\n",
    "  hr_ax.plot(HR[i], label = i, color = colors[num])\n",
    "  ndcg_ax.plot(NDCG[i], label = i, color = colors[num])\n",
    "  hr_ax.plot([10],[HR[i][10]], marker = \"o\", color = colors[num], label = \"HR@10\")\n",
    "  ndcg_ax.plot([10],[NDCG[i][10]], marker = \"o\", color = colors[num], label = \"NDCG@10\")\n",
    "\n",
    "\n",
    "hr_ax.set_title(\"Hit Rate while number of recommendations vary\")\n",
    "ndcg_ax.set_title(\"NDCG Rate while number of recommendations vary\")\n",
    "hr_ax.set_ylabel(\"HR\")\n",
    "hr_ax.set_xlabel(\"K value\")\n",
    "ndcg_ax.set_ylabel(\"NDCG\")\n",
    "ndcg_ax.set_xlabel(\"K value\")\n",
    "hr_ax.legend()\n",
    "ndcg_ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3 - Metrics Evaluation while num factors vary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_factors = [8, 16, 32, 64]\n",
    "#do not change the order of HR and NDCG matrices\n",
    "model_names = [\"GMF Model\", \"MLP Model\", \"NeuMF model\"]\n",
    "mlp_models, gmf_models, nmf_models = {}, {}, {}\n",
    "\n",
    "#I create a dict containing as the key the num_factor and as a value the relative model \n",
    "for num in num_factors:\n",
    "    mlp_models[num] = torch.load(MODEL_PATH + \"MLP{}.pt\".format(num), map_location = device)\n",
    "    gmf_models[num] = torch.load(MODEL_PATH + \"GMF{}.pt\".format(num), map_location = device)\n",
    "    nmf_models[num] = torch.load(MODEL_PATH + \"NeuMF{}.pt\".format(num), map_location = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HR, NDCG = list([]), list([])\n",
    "\n",
    "#for each num factor and for each model I compute the provided metrics on the test set\n",
    "for num in tqdm(num_factors):\n",
    "    hr_gmf, ndcg_gmf = metrics(gmf_models[num], test_loader, 10, device)\n",
    "    hr_mlp, ndcg_mlp = metrics(mlp_models[num], test_loader, 10, device)\n",
    "    hr_nmf, ndcg_nmf = metrics(nmf_models[num], test_loader, 10, device)\n",
    "    HR.append([hr_gmf, hr_mlp, hr_nmf])\n",
    "    NDCG.append([ndcg_gmf, ndcg_mlp, ndcg_nmf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here I transpose the matrices because I need to have on the same row the results of the metrics for the same model (in order to construct the bar plot)\n",
    "HR = np.array(HR).T\n",
    "NDCG = np.array(NDCG).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.container import BarContainer \n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, layout='constrained', figsize = (10,12))\n",
    "x = np.arange(HR.shape[1])\n",
    "offset = -0.2\n",
    "for i in range(len(HR)):\n",
    "    rects1 = ax1.bar(x + offset, np.round(HR[i], 3), width=0.2, label = model_names[i])\n",
    "    rects2 = ax2.bar(x + offset, np.round(NDCG[i], 3), width=0.2, label = model_names[i])\n",
    "    ax1.bar_label(rects1, padding = 2)\n",
    "    ax2.bar_label(rects2, padding = 2)\n",
    "    offset += 0.2\n",
    "\n",
    "ax1.set_ylim(min(HR.reshape(-1,1)) - 0.05, max(HR.reshape(-1,1)) + 0.05)\n",
    "ax2.set_ylim(min(NDCG.reshape(-1,1)) - 0.05, max(NDCG.reshape(-1,1)) + 0.05)\n",
    "\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_ylabel(\"HR\")\n",
    "ax1.set_xticklabels(num_factors)\n",
    "ax1.set_xlabel(\"num factors\")\n",
    "\n",
    "\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_ylabel(\"NDCG\")\n",
    "ax2.set_xlabel(\"num factors\")\n",
    "ax2.set_xticklabels(num_factors)\n",
    "\n",
    "ax1.set_title(\"Hit Rate\")\n",
    "ax2.set_title(\"Normalized Discounted Cumulative Gain\")\n",
    "\n",
    "ax1.legend()\n",
    "ax2.legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HyperParameter Tuning With RayTune\n",
    "If there is a need for a more detailed description of how to use and configure the Hyper Parameter Tuning procedure we will provide them in the next version of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We first need to define a config dict that defines the search space of the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"lr\": tune.choice([0.005, 0.015, 0.01, 0.001]),\n",
    "    \"batch_size\": tune.choice([128, 256, 512]),\n",
    "    \"num_factors\": tune.choice([8, 16, 32]),\n",
    "    \"epochs\": tune.choice([15, 30, 40]),\n",
    "    \"out\": True,\n",
    "    \"layers\": tune.choice([[32, 16, 8], [64, 32, 8]]),\n",
    "    \"dropout\": np.random.choice([0, 0.2, 0.5], size=(7, 20), p=[0.5, 0.3, 0.2])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the scheduler and running the HT procedure\n",
    "There is a paper related to the ASAH Scheduler at :https://arxiv.org/abs/1810.05934"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = ASHAScheduler(\n",
    "        metric=\"loss\",\n",
    "        mode=\"min\",\n",
    "        max_t=40,\n",
    "        grace_period=5,\n",
    "        reduction_factor=2)\n",
    "\n",
    "reporter = CLIReporter(\n",
    "        metric_columns=[\"loss\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Function\n",
    "We have to define a train function that takes as argument the config file.\n",
    "This function will:\n",
    "- Initialize NeuMF model with the specified config parameters (num factors, layers, dropout vector for the different layers)\n",
    "- Initialize Train and validation dataloaders\n",
    "- Train the model on the Train dataloader\n",
    "- Evaluate the validation loss and report it to RayTune\n",
    "- Save the best model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trains and save the model and returns the lists of train_loss and validation_loss\n",
    "def train_NeuMF(config):\n",
    "    # model = NeuMF_parametric_dropout(config['num_factors'], num_users, num_items, config['layers'], config['dropout'])\n",
    "    model = NeuMF(\n",
    "        config[\"num_factors\"], num_users, num_items, config[\"layers\"], config[\"dropout\"]\n",
    "    )\n",
    "\n",
    "    train_loader, valid_loader, _, _ = data_loaders_from_data(\n",
    "        PATH, batch_size=config[\"batch_size\"]\n",
    "    )\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    loss_function = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
    "\n",
    "    # train, evaluation\n",
    "    best_valid_loss = math.inf\n",
    "    train_loss, valid_loss = [], []\n",
    "\n",
    "    for epoch in range(config[\"epochs\"]):\n",
    "        model.train()  # Enable dropout (if present).\n",
    "\n",
    "        intermediate_train_loss = []\n",
    "        for user, item, label in train_loader:\n",
    "            user = user.to(device)\n",
    "            item = item.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            prediction = model(user, item)\n",
    "            loss = loss_function(prediction, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            intermediate_train_loss.append(loss.cpu().detach().item())\n",
    "\n",
    "        train_loss.append(np.mean(intermediate_train_loss))\n",
    "        model.eval()\n",
    "\n",
    "        intermediate_valid_loss = []\n",
    "        for user, item, label in valid_loader:\n",
    "            user = user.to(device)\n",
    "            item = item.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            prediction = model(user, item)\n",
    "            loss = loss_function(prediction, label)\n",
    "\n",
    "            intermediate_valid_loss.append(loss.cpu().detach().item())\n",
    "\n",
    "        valid_loss.append(np.mean(intermediate_valid_loss))\n",
    "        if valid_loss[epoch] < best_valid_loss:\n",
    "            best_valid_loss = valid_loss[epoch]\n",
    "            if config[\"out\"]:\n",
    "                if not os.path.exists(MODEL_PATH):\n",
    "                    os.mkdir(MODEL_PATH)\n",
    "                torch.save(\n",
    "                    model,\n",
    "                    f\"{MODEL_PATH}{model.__class__.__name__}{model.num_factors}.pt\",\n",
    "                )\n",
    "\n",
    "        with tune.checkpoint_dir(epoch) as checkpoint_dir:\n",
    "            path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "            torch.save((model.state_dict(), optimizer.state_dict()), path)\n",
    "\n",
    "        tune.report(loss=valid_loss[epoch])\n",
    "\n",
    "    losses = pd.DataFrame({\"train loss\": train_loss, \"valid loss\": valid_loss})\n",
    "    losses.to_csv(f\"{MODEL_PATH}{model.__class__.__name__}{model.num_factors}.csv\")\n",
    "    return train_loss, valid_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = tune.run(\n",
    "    train_NeuMF,\n",
    "    local_dir=\"./checkpoints/\",\n",
    "    resources_per_trial={\"cpu\": 4},\n",
    "    config=config,\n",
    "    num_samples=10,\n",
    "    scheduler=scheduler,\n",
    "    progress_reporter=reporter,\n",
    "    checkpoint_at_end=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n",
    "best_checkpoint_dir = best_trial.checkpoint.dir_or_data\n",
    "model_state, optimizer_state = torch.load(os.path.join(\n",
    "    best_checkpoint_dir, \"checkpoint\"))\n",
    "\n",
    "best_trained_model = NeuMF(best_trial.config[\"num_factors\"], num_users, num_items, best_trial.config[\"layers\"], best_trial.config['dropout']) \n",
    "\n",
    "best_trained_model.load_state_dict(model_state)\n",
    "torch.save(best_trained_model, '{}{}{}_best_so_far.pt'.format(MODEL_PATH, best_trained_model.__class__.__name__, best_trained_model.factor_num))\n",
    "\n",
    "best_trial.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reporting the best configuration in our tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_config = {\n",
    "    \"lr\": 0.01,\n",
    "    \"batch_size\": 256,\n",
    "    \"num_factors\": 8,\n",
    "    \"epochs\": 20,\n",
    "    \"out\": True,\n",
    "    \"layers\": (64, 16, 8),\n",
    "    \"dropout\": (0, 0, 0, 0, 0, 0, 0.5),\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
