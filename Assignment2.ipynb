{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team composed by:\n",
    "- Agostara NicolÃ²\n",
    "- Fratti Giorgio\n",
    "- Fusillo Antonio\n",
    "- Protti Edoardo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from utility import Rating_Datset\n",
    "from neumf import NeuMF\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting variables and functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PATHs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"./ml-100k/u.data\" \n",
    "MODEL_PATH = \"./models_2/\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"seed\": 42,\n",
    "    \"lr\": 0.001,\n",
    "    \"dropout\": 0.2,\n",
    "    \"batch_size\": 256,\n",
    "    \"epochs\": 25,\n",
    "    \"top_k\": 10,\n",
    "    \"num_factors\": 32,\n",
    "    \"layers\": (32, 16, 8),\n",
    "    \"out\": True,\n",
    "    \"num_ng\": 4,\n",
    "    \"num_ng_test\": 100\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HIT RATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hit(ng_item, pred_items):\n",
    "    return 1 if ng_item in pred_items else 0\n",
    "\n",
    "def metrics(model, test_loader, top_k, device):\n",
    "    HR = []\n",
    "\n",
    "    for user, item, label in test_loader:\n",
    "        user = user.to(device)\n",
    "        item = item.to(device)\n",
    "\n",
    "        predictions = model(user, item)\n",
    "        _, indices = torch.topk(predictions, top_k)\n",
    "        recommends = torch.take(item, indices).cpu().numpy().tolist()\n",
    "\n",
    "        ng_item = item[0].item()  # leave one-out evaluation has only one item per user\n",
    "        HR.append(hit(ng_item, recommends))\n",
    "\n",
    "    return np.mean(HR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NCF_Data(object):\n",
    "\t\"\"\"\n",
    "\tConstruct Dataset for NCF\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, ratings):\n",
    "\t\tself.ratings = ratings\n",
    "\t\tself.num_ng = args[\"num_ng\"]\n",
    "\t\tself.num_ng_test = args[\"num_ng_test\"]\n",
    "\t\tself.batch_size = args[\"batch_size\"]\n",
    "\n",
    "\t\tself.preprocess_ratings = self._reindex(self.ratings)\n",
    "\n",
    "\t\tself.user_pool = set(self.ratings['user_id'].unique())\n",
    "\t\tself.item_pool = set(self.ratings['item_id'].unique())\n",
    "\n",
    "\t\tself.train_ratings, self.test_ratings = self._leave_one_out(self.preprocess_ratings)\n",
    "\t\tself.negatives = self._negative_sampling(self.preprocess_ratings)\n",
    "\n",
    "\t\n",
    "\tdef _reindex(self, ratings):\n",
    "\t\t\"\"\"\n",
    "\t\tProcess dataset to reindex userID and itemID, also set rating as binary feedback\n",
    "\t\t\"\"\"\n",
    "\t\tuser_list = list(ratings['user_id'].drop_duplicates())\n",
    "\t\tuser2id = {w: i for i, w in enumerate(user_list)}\n",
    "\n",
    "\t\titem_list = list(ratings['item_id'].drop_duplicates())\n",
    "\t\titem2id = {w: i for i, w in enumerate(item_list)}\n",
    "\n",
    "\t\tratings['user_id'] = ratings['user_id'].apply(lambda x: user2id[x])\n",
    "\t\tratings['item_id'] = ratings['item_id'].apply(lambda x: item2id[x])\n",
    "\t\tratings['rating'] = ratings['rating'].apply(lambda x: float(x > 0))\n",
    "\t\treturn ratings\n",
    "\n",
    "\tdef _leave_one_out(self, ratings):\n",
    "\t\t\"\"\"\n",
    "\t\tleave-one-out evaluation protocol in paper https://www.comp.nus.edu.sg/~xiangnan/papers/ncf.pdf\n",
    "\t\t\"\"\"\n",
    "\t\tratings['rank_latest'] = ratings.groupby(['user_id'])['timestamp'].rank(method='first', ascending=False)\n",
    "\t\ttest = ratings.loc[ratings['rank_latest'] == 1]\n",
    "\t\ttrain = ratings.loc[ratings['rank_latest'] > 1]\n",
    "\t\tassert train['user_id'].nunique()==test['user_id'].nunique(), 'Not Match Train User with Test User'\n",
    "\t\treturn train[['user_id', 'item_id', 'rating']], test[['user_id', 'item_id', 'rating']]\n",
    "\n",
    "\tdef _negative_sampling(self, ratings):\n",
    "\t\tinteract_status = (\n",
    "\t\t\tratings.groupby('user_id')['item_id']\n",
    "\t\t\t.apply(set)\n",
    "\t\t\t.reset_index()\n",
    "\t\t\t.rename(columns={'item_id': 'interacted_items'}))\n",
    "\t\tinteract_status['negative_items'] = interact_status['interacted_items'].apply(lambda x: self.item_pool - x)\n",
    "\t\tinteract_status['negative_samples'] = interact_status['negative_items'].apply(lambda x: random.sample(x, self.num_ng_test))\n",
    "\t\treturn interact_status[['user_id', 'negative_items', 'negative_samples']]\n",
    "\n",
    "\tdef get_train_instance(self):\n",
    "\t\tusers, items, ratings = [], [], []\n",
    "\t\ttrain_ratings = pd.merge(self.train_ratings, self.negatives[['user_id', 'negative_items']], on='user_id')\n",
    "\t\ttrain_ratings['negatives'] = train_ratings['negative_items'].apply(lambda x: random.sample(x, self.num_ng))\n",
    "\t\tfor row in train_ratings.itertuples():\n",
    "\t\t\tusers.append(int(row.user_id))\n",
    "\t\t\titems.append(int(row.item_id))\n",
    "\t\t\tratings.append(float(row.rating))\n",
    "\t\t\tfor i in range(self.num_ng):\n",
    "\t\t\t\tusers.append(int(row.user_id))\n",
    "\t\t\t\titems.append(int(row.negatives[i]))\n",
    "\t\t\t\tratings.append(float(0))  # negative samples get 0 rating\n",
    "\t\tdataset = Rating_Datset(\n",
    "\t\t\tuser_list=users,\n",
    "\t\t\titem_list=items,\n",
    "\t\t\trating_list=ratings)\n",
    "\t\treturn DataLoader(dataset, batch_size=self.batch_size, shuffle=True, num_workers=1)\n",
    "\n",
    "\tdef get_test_instance(self):\n",
    "\t\tusers, items, ratings = [], [], []\n",
    "\t\ttest_ratings = pd.merge(self.test_ratings, self.negatives[['user_id', 'negative_samples']], on='user_id')\n",
    "\t\tfor row in test_ratings.itertuples():\n",
    "\t\t\tusers.append(int(row.user_id))\n",
    "\t\t\titems.append(int(row.item_id))\n",
    "\t\t\tratings.append(float(row.rating))\n",
    "\t\t\tfor i in getattr(row, 'negative_samples'):\n",
    "\t\t\t\tusers.append(int(row.user_id))\n",
    "\t\t\t\titems.append(int(i))\n",
    "\t\t\t\tratings.append(float(0))\n",
    "\t\tdataset = Rating_Datset(\n",
    "\t\t\tuser_list=users,\n",
    "\t\t\titem_list=items,\n",
    "\t\t\trating_list=ratings)\n",
    "\t\treturn DataLoader(dataset, batch_size=self.num_ng_test+1, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model: nn.Module, train_loader: DataLoader, test_loader: DataLoader):\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Loss and optimizer\n",
    "    loss_function = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args[\"lr\"])\n",
    "\n",
    "    best_hr = 0\n",
    "    \n",
    "    # Train cycle\n",
    "    for epoch in range(args[\"epochs\"]+1):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Train step\n",
    "        model.train()\n",
    "\n",
    "        for user, item, label in train_loader:\n",
    "            user = user.to(device)\n",
    "            item = item.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            # Zero grad\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Prediction\n",
    "            prediction = model(user, item)\n",
    "            loss = loss_function(prediction, label)\n",
    "            \n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Eval metrics\n",
    "        model.eval()\n",
    "        HR = metrics(model, test_loader, args[\"top_k\"], device)\n",
    "\n",
    "        # Print metrics and time elapsed\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(\n",
    "            \"Epoch {:03d} |\".format(epoch)\n",
    "            + \" HR: {:.3f} |\".format(np.mean(HR))\n",
    "            + \" time: \"\n",
    "            + time.strftime(\"%H: %M: %S\", time.gmtime(elapsed_time)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # If best model, save it\n",
    "        if HR > best_hr:\n",
    "            best_hr, best_epoch = HR, epoch\n",
    "            if args[\"out\"]:\n",
    "                if not os.path.exists(MODEL_PATH):\n",
    "                    os.mkdir(MODEL_PATH)\n",
    "                torch.save(\n",
    "                    model, \"{}{}{}.pt\".format(MODEL_PATH, model.__class__.__name__, model.num_factors)\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fr/_8m6478904jb8v5wdgyfm_j80000gn/T/ipykernel_31873/3176161701.py:52: DeprecationWarning: Sampling from a set deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version.\n",
      "  interact_status['negative_samples'] = interact_status['negative_items'].apply(lambda x: random.sample(x, self.num_ng_test))\n",
      "/var/folders/fr/_8m6478904jb8v5wdgyfm_j80000gn/T/ipykernel_31873/3176161701.py:58: DeprecationWarning: Sampling from a set deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version.\n",
      "  train_ratings['negatives'] = train_ratings['negative_items'].apply(lambda x: random.sample(x, self.num_ng))\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "ml_100k = pd.read_csv(\n",
    "\tPATH, \n",
    "\tsep=\"\\t\", \n",
    "\tnames = ['user_id', 'item_id', 'rating', 'timestamp'], \n",
    "\tengine='python')\n",
    "\n",
    "# set the num_users, items\n",
    "num_users = ml_100k['user_id'].nunique()+1\n",
    "num_items = ml_100k['item_id'].nunique()+1\n",
    "\n",
    "# construct the train and test datasets\n",
    "data = NCF_Data(ml_100k)\n",
    "train_loader = data.get_train_instance()\n",
    "test_loader = data.get_test_instance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ---\n",
    "### TASK 1: Train the GMF and MLP separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model from Assignment 1:\n",
    "best = torch.load(\"models/NeuMF64_tuned.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmf_factors = best.gmf_user_embed[0].weight.shape[1] # num_factors for GMF model\n",
    "class GMF(nn.Module):\n",
    "    def __init__(self, num_users, num_items):\n",
    "        super(GMF, self).__init__()\n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "        self.num_factors = gmf_factors\n",
    "\n",
    "        self.embedding_user = nn.Embedding(\n",
    "            num_embeddings=self.num_users, embedding_dim=self.num_factors\n",
    "        )\n",
    "        self.embedding_item = nn.Embedding(\n",
    "            num_embeddings=self.num_items, embedding_dim=self.num_factors\n",
    "        )\n",
    "\n",
    "        self.affine_output = nn.Linear(in_features=self.num_factors, out_features=1)\n",
    "        self.logistic = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, user_indices, item_indices):\n",
    "        user_embedding = self.embedding_user(user_indices)\n",
    "        item_embedding = self.embedding_item(item_indices)\n",
    "        element_product = torch.mul(user_embedding, item_embedding)\n",
    "        logits = self.affine_output(element_product)\n",
    "        rating = self.logistic(logits)\n",
    "        return rating.squeeze()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and saving the GMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmf_model = GMF(num_users, num_items)\n",
    "train_model(gmf_model, train_loader, test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_factors = best.mlp_user_embed[0].weight.shape[1] # num_factors for GMF model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, num_users, num_items):\n",
    "        super(MLP, self).__init__()\n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "        self.num_factors = mlp_factors\n",
    "\n",
    "        self.embedding_user = nn.Embedding(\n",
    "            num_embeddings=num_users, embedding_dim=mlp_factors\n",
    "        )\n",
    "        self.embedding_item = nn.Embedding(\n",
    "            num_embeddings=num_items, embedding_dim=mlp_factors\n",
    "        )\n",
    "\n",
    "        layer_sizes = args[\"layers\"]\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(mlp_factors * 2, layer_sizes[0]))\n",
    "        layers.append(nn.ReLU())\n",
    "        for in_size, out_size in zip(layer_sizes[:-1], layer_sizes[1:]):\n",
    "            layers.append(nn.Linear(in_size, out_size))\n",
    "            layers.append(nn.ReLU())\n",
    "        self.mlp_fc = nn.Sequential(*layers)\n",
    "        self.mlp_fc.add_module(\"affine\", nn.Linear(layer_sizes[-1], 1))\n",
    "        self.mlp_fc.add_module(\"logit\", nn.Sigmoid())\n",
    "\n",
    "    def forward(self, user_indices, item_indices):\n",
    "        user_embedding = self.embedding_user(user_indices)\n",
    "        item_embedding = self.embedding_item(item_indices)\n",
    "        vector = torch.cat([user_embedding, item_embedding], dim=-1)\n",
    "        rating = self.mlp_fc(vector)\n",
    "        return rating.squeeze()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and saving the MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000 | HR: 0.401 | time: 00: 00: 21\n",
      "Epoch 001 | HR: 0.408 | time: 00: 00: 22\n",
      "Epoch 002 | HR: 0.407 | time: 00: 00: 23\n",
      "Epoch 003 | HR: 0.424 | time: 00: 00: 22\n",
      "Epoch 004 | HR: 0.444 | time: 00: 00: 21\n",
      "Epoch 005 | HR: 0.473 | time: 00: 00: 24\n",
      "Epoch 006 | HR: 0.496 | time: 00: 00: 22\n",
      "Epoch 007 | HR: 0.514 | time: 00: 00: 23\n",
      "Epoch 008 | HR: 0.521 | time: 00: 00: 23\n",
      "Epoch 009 | HR: 0.546 | time: 00: 00: 21\n",
      "Epoch 010 | HR: 0.557 | time: 00: 00: 21\n",
      "Epoch 011 | HR: 0.557 | time: 00: 00: 22\n",
      "Epoch 012 | HR: 0.558 | time: 00: 00: 22\n",
      "Epoch 013 | HR: 0.568 | time: 00: 00: 21\n",
      "Epoch 014 | HR: 0.559 | time: 00: 00: 21\n",
      "Epoch 015 | HR: 0.566 | time: 00: 00: 21\n",
      "Epoch 016 | HR: 0.562 | time: 00: 00: 22\n",
      "Epoch 017 | HR: 0.575 | time: 00: 00: 22\n",
      "Epoch 018 | HR: 0.564 | time: 00: 00: 23\n",
      "Epoch 019 | HR: 0.563 | time: 00: 00: 22\n",
      "Epoch 020 | HR: 0.561 | time: 00: 00: 21\n",
      "Epoch 021 | HR: 0.565 | time: 00: 00: 22\n",
      "Epoch 022 | HR: 0.571 | time: 00: 00: 22\n",
      "Epoch 023 | HR: 0.566 | time: 00: 00: 21\n",
      "Epoch 024 | HR: 0.575 | time: 00: 00: 21\n",
      "Epoch 025 | HR: 0.573 | time: 00: 00: 21\n"
     ]
    }
   ],
   "source": [
    "mlp_model = MLP(num_users, num_items)\n",
    "train_model(mlp_model, train_loader, test_loader) # Stopped after 17 epochs to prevent overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NeuMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuMF_class(nn.Module):\n",
    "    def __init__(self, num_factors_gmf, num_factors_mlp, num_users, num_items):\n",
    "\n",
    "        super(NeuMF_class, self).__init__()\n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "        self.num_factors_gmf = gmf_factors\n",
    "        self.num_factors_mlp = mlp_factors\n",
    "        self.num_factors = max(num_factors_mlp, num_factors_gmf) #just for saving model name purpose\n",
    "\n",
    "        # GMF component\n",
    "        self.gmf_user_embed = nn.Embedding(num_embeddings=self.num_users, embedding_dim=self.num_factors_gmf)\n",
    "        self.gmf_item_embed = nn.Embedding(num_embeddings=self.num_items, embedding_dim=self.num_factors_gmf)\n",
    "        self.gmf_affine = nn.Linear(\n",
    "            in_features=self.num_factors_gmf, out_features=self.num_factors_gmf, bias=True\n",
    "        )\n",
    "\n",
    "        # MLP component\n",
    "        self.mlp_user_embed = nn.Embedding(num_embeddings=self.num_users, embedding_dim=self.num_factors_mlp)\n",
    "        self.mlp_item_embed = nn.Embedding(num_embeddings=self.num_items, embedding_dim=self.num_factors_mlp)\n",
    "\n",
    "        self.mlp_fc = nn.Sequential(\n",
    "            #We have considered as possible num_factors [8, 16, 32, 64] so this structure works\n",
    "            nn.Linear(2 * self.num_factors_mlp, self.num_factors_mlp),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(self.num_factors_mlp, int(self.num_factors_mlp / 2)),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(int(self.num_factors_mlp / 2), int(self.num_factors_mlp / 4)),  \n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Combine models\n",
    "        input_dim = self.num_factors_gmf + int(self.num_factors_mlp / 4)\n",
    "        self.mixing_layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, int(input_dim / 2)),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(int(input_dim / 2), int(input_dim / 4)),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(int(input_dim / 4), 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, user_indices, item_indices):\n",
    "        # GMF forward\n",
    "        user_embedding_gmf = self.gmf_user_embed(user_indices)\n",
    "        item_embedding_gmf = self.gmf_item_embed(item_indices)\n",
    "\n",
    "        element_product = torch.mul(user_embedding_gmf, item_embedding_gmf)\n",
    "        ratings_gmf = self.gmf_affine(element_product)\n",
    "\n",
    "        # MLP forward\n",
    "        user_embedding_mlp = self.mlp_user_embed(user_indices)\n",
    "        item_embedding_mlp = self.mlp_item_embed(item_indices)\n",
    "\n",
    "        vector = torch.cat((user_embedding_mlp, item_embedding_mlp), dim=-1)\n",
    "        ratings_mlp = self.mlp_fc(vector)\n",
    "\n",
    "        # Combine\n",
    "        ratings = torch.cat((ratings_gmf, ratings_mlp), dim=1)\n",
    "        return self.mixing_layers(ratings).squeeze()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuMF_f = NeuMF_class(gmf_factors, mlp_factors, num_users, num_items)\n",
    "neuMF_nof = NeuMF_class(gmf_factors, mlp_factors, num_users, num_items)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 1.6831,  0.4672,  0.8025,  ..., -0.8291, -0.4639,  0.9350],\n",
       "        [-0.0033, -0.4051, -0.8245,  ..., -2.2704,  0.6260,  0.5048],\n",
       "        [ 0.8177, -0.9323, -1.4887,  ..., -0.5728,  1.4165,  0.0176],\n",
       "        ...,\n",
       "        [-0.7927,  0.9772,  1.5114,  ..., -1.5120,  1.2837,  0.6163],\n",
       "        [-0.8549, -1.4636,  0.0792,  ..., -1.8420,  1.5156,  1.4550],\n",
       "        [-0.1027, -1.4767, -1.4588,  ...,  1.4242,  1.0193,  1.3637]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmf_model.embedding_user.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(1683, 8)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neuMF_f.gmf_user_embed.weight = gmf_model.embedding_user.weight\n",
    "neuMF_f.gmf_user_embed.requires_grad_(False)\n",
    "neuMF_f.gmf_item_embed.weight = gmf_model.embedding_item.weight\n",
    "neuMF_f.gmf_item_embed.requires_grad_(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=128, out_features=32, bias=True)\n",
       "  (1): Linear(in_features=32, out_features=16, bias=True)\n",
       "  (2): ReLU()\n",
       "  (3): Linear(in_features=16, out_features=8, bias=True)\n",
       "  (4): ReLU()\n",
       "  (affine): Linear(in_features=8, out_features=1, bias=True)\n",
       "  (logit): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_model.mlp_fc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(1683, 64)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neuMF_f.mlp_user_embed.weight = mlp_model.embedding_user.weight\n",
    "neuMF_f.mlp_user_embed.requires_grad_(False)\n",
    "neuMF_f.mlp_item_embed.weight = mlp_model.embedding_item.weight\n",
    "neuMF_f.mlp_item_embed.requires_grad_(False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit ('miniforge3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d29b108309575d5ffeaef586407ef10816dbaacc527dc95f14090a22594fdff0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
