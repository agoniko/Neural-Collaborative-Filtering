{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team composed by:\n",
    "- Agostara NicolÃ²\n",
    "- Fratti Giorgio\n",
    "- Fusillo Antonio\n",
    "- Protti Edoardo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from utility import Rating_Datset\n",
    "from neumf import NeuMF\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting variables and functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PATHs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"./ml-100k/u.data\" \n",
    "MODEL_PATH = \"./models_2/\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"seed\": 42,\n",
    "    \"lr\": 0.001,\n",
    "    \"dropout\": 0.2,\n",
    "    \"batch_size\": 256,\n",
    "    \"epochs\": 25,\n",
    "    \"top_k\": 10,\n",
    "    \"num_factors\": 32,\n",
    "    \"layers\": (64, 32, 16),\n",
    "    \"out\": True,\n",
    "    \"num_ng\": 4,\n",
    "    \"num_ng_test\": 100\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HIT RATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hit(ng_item, pred_items):\n",
    "    return 1 if ng_item in pred_items else 0\n",
    "\n",
    "def metrics(model, test_loader, top_k, device):\n",
    "    HR = []\n",
    "\n",
    "    for user, item, label in test_loader:\n",
    "        user = user.to(device)\n",
    "        item = item.to(device)\n",
    "\n",
    "        predictions = model(user, item)\n",
    "        _, indices = torch.topk(predictions, top_k)\n",
    "        recommends = torch.take(item, indices).cpu().numpy().tolist()\n",
    "\n",
    "        ng_item = item[0].item()  # leave one-out evaluation has only one item per user\n",
    "        HR.append(hit(ng_item, recommends))\n",
    "\n",
    "    return np.mean(HR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NCF_Data(object):\n",
    "\t\"\"\"\n",
    "\tConstruct Dataset for NCF\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, ratings):\n",
    "\t\tself.ratings = ratings\n",
    "\t\tself.num_ng = args[\"num_ng\"]\n",
    "\t\tself.num_ng_test = args[\"num_ng_test\"]\n",
    "\t\tself.batch_size = args[\"batch_size\"]\n",
    "\n",
    "\t\tself.preprocess_ratings = self._reindex(self.ratings)\n",
    "\n",
    "\t\tself.user_pool = set(self.ratings['user_id'].unique())\n",
    "\t\tself.item_pool = set(self.ratings['item_id'].unique())\n",
    "\n",
    "\t\tself.train_ratings, self.test_ratings = self._leave_one_out(self.preprocess_ratings)\n",
    "\t\tself.negatives = self._negative_sampling(self.preprocess_ratings)\n",
    "\n",
    "\t\n",
    "\tdef _reindex(self, ratings):\n",
    "\t\t\"\"\"\n",
    "\t\tProcess dataset to reindex userID and itemID, also set rating as binary feedback\n",
    "\t\t\"\"\"\n",
    "\t\tuser_list = list(ratings['user_id'].drop_duplicates())\n",
    "\t\tuser2id = {w: i for i, w in enumerate(user_list)}\n",
    "\n",
    "\t\titem_list = list(ratings['item_id'].drop_duplicates())\n",
    "\t\titem2id = {w: i for i, w in enumerate(item_list)}\n",
    "\n",
    "\t\tratings['user_id'] = ratings['user_id'].apply(lambda x: user2id[x])\n",
    "\t\tratings['item_id'] = ratings['item_id'].apply(lambda x: item2id[x])\n",
    "\t\tratings['rating'] = ratings['rating'].apply(lambda x: float(x > 0))\n",
    "\t\treturn ratings\n",
    "\n",
    "\tdef _leave_one_out(self, ratings):\n",
    "\t\t\"\"\"\n",
    "\t\tleave-one-out evaluation protocol in paper https://www.comp.nus.edu.sg/~xiangnan/papers/ncf.pdf\n",
    "\t\t\"\"\"\n",
    "\t\tratings['rank_latest'] = ratings.groupby(['user_id'])['timestamp'].rank(method='first', ascending=False)\n",
    "\t\ttest = ratings.loc[ratings['rank_latest'] == 1]\n",
    "\t\ttrain = ratings.loc[ratings['rank_latest'] > 1]\n",
    "\t\tassert train['user_id'].nunique()==test['user_id'].nunique(), 'Not Match Train User with Test User'\n",
    "\t\treturn train[['user_id', 'item_id', 'rating']], test[['user_id', 'item_id', 'rating']]\n",
    "\n",
    "\tdef _negative_sampling(self, ratings):\n",
    "\t\tinteract_status = (\n",
    "\t\t\tratings.groupby('user_id')['item_id']\n",
    "\t\t\t.apply(set)\n",
    "\t\t\t.reset_index()\n",
    "\t\t\t.rename(columns={'item_id': 'interacted_items'}))\n",
    "\t\tinteract_status['negative_items'] = interact_status['interacted_items'].apply(lambda x: self.item_pool - x)\n",
    "\t\tinteract_status['negative_samples'] = interact_status['negative_items'].apply(lambda x: random.sample(x, self.num_ng_test))\n",
    "\t\treturn interact_status[['user_id', 'negative_items', 'negative_samples']]\n",
    "\n",
    "\tdef get_train_instance(self):\n",
    "\t\tusers, items, ratings = [], [], []\n",
    "\t\ttrain_ratings = pd.merge(self.train_ratings, self.negatives[['user_id', 'negative_items']], on='user_id')\n",
    "\t\ttrain_ratings['negatives'] = train_ratings['negative_items'].apply(lambda x: random.sample(x, self.num_ng))\n",
    "\t\tfor row in train_ratings.itertuples():\n",
    "\t\t\tusers.append(int(row.user_id))\n",
    "\t\t\titems.append(int(row.item_id))\n",
    "\t\t\tratings.append(float(row.rating))\n",
    "\t\t\tfor i in range(self.num_ng):\n",
    "\t\t\t\tusers.append(int(row.user_id))\n",
    "\t\t\t\titems.append(int(row.negatives[i]))\n",
    "\t\t\t\tratings.append(float(0))  # negative samples get 0 rating\n",
    "\t\tdataset = Rating_Datset(\n",
    "\t\t\tuser_list=users,\n",
    "\t\t\titem_list=items,\n",
    "\t\t\trating_list=ratings)\n",
    "\t\treturn DataLoader(dataset, batch_size=self.batch_size, shuffle=True, num_workers=1)\n",
    "\n",
    "\tdef get_test_instance(self):\n",
    "\t\tusers, items, ratings = [], [], []\n",
    "\t\ttest_ratings = pd.merge(self.test_ratings, self.negatives[['user_id', 'negative_samples']], on='user_id')\n",
    "\t\tfor row in test_ratings.itertuples():\n",
    "\t\t\tusers.append(int(row.user_id))\n",
    "\t\t\titems.append(int(row.item_id))\n",
    "\t\t\tratings.append(float(row.rating))\n",
    "\t\t\tfor i in getattr(row, 'negative_samples'):\n",
    "\t\t\t\tusers.append(int(row.user_id))\n",
    "\t\t\t\titems.append(int(i))\n",
    "\t\t\t\tratings.append(float(0))\n",
    "\t\tdataset = Rating_Datset(\n",
    "\t\t\tuser_list=users,\n",
    "\t\t\titem_list=items,\n",
    "\t\t\trating_list=ratings)\n",
    "\t\treturn DataLoader(dataset, batch_size=self.num_ng_test+1, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model: nn.Module, train_loader: DataLoader, test_loader: DataLoader, args):\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Loss and optimizer\n",
    "    loss_function = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args[\"lr\"])\n",
    "\n",
    "    best_hr = 0\n",
    "    \n",
    "    # Train cycle\n",
    "    for epoch in range(args[\"epochs\"]+1):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Train step\n",
    "        model.train()\n",
    "\n",
    "        for user, item, label in train_loader:\n",
    "            user = user.to(device)\n",
    "            item = item.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            # Zero grad\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Prediction\n",
    "            prediction = model(user, item)\n",
    "            loss = loss_function(prediction, label)\n",
    "            \n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Eval metrics\n",
    "        model.eval()\n",
    "        HR = metrics(model, test_loader, args[\"top_k\"], device)\n",
    "\n",
    "        # Print metrics and time elapsed\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(\n",
    "            \"Epoch {:03d} |\".format(epoch)\n",
    "            + \" HR: {:.3f} |\".format(np.mean(HR))\n",
    "            + \" time: \"\n",
    "            + time.strftime(\"%H: %M: %S\", time.gmtime(elapsed_time)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # If best model, save it\n",
    "        if HR > best_hr:\n",
    "            best_hr, best_epoch = HR, epoch\n",
    "            if args[\"out\"]:\n",
    "                if not os.path.exists(MODEL_PATH):\n",
    "                    os.mkdir(MODEL_PATH)\n",
    "                torch.save(\n",
    "                    model, \"{}{}{}.pt\".format(MODEL_PATH, model.__class__.__name__, model.num_factors)\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fr/_8m6478904jb8v5wdgyfm_j80000gn/T/ipykernel_38841/3176161701.py:52: DeprecationWarning: Sampling from a set deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version.\n",
      "  interact_status['negative_samples'] = interact_status['negative_items'].apply(lambda x: random.sample(x, self.num_ng_test))\n",
      "/var/folders/fr/_8m6478904jb8v5wdgyfm_j80000gn/T/ipykernel_38841/3176161701.py:58: DeprecationWarning: Sampling from a set deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version.\n",
      "  train_ratings['negatives'] = train_ratings['negative_items'].apply(lambda x: random.sample(x, self.num_ng))\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "ml_100k = pd.read_csv(\n",
    "\tPATH, \n",
    "\tsep=\"\\t\", \n",
    "\tnames = ['user_id', 'item_id', 'rating', 'timestamp'], \n",
    "\tengine='python')\n",
    "\n",
    "# set the num_users, items\n",
    "num_users = ml_100k['user_id'].nunique()+1\n",
    "num_items = ml_100k['item_id'].nunique()+1\n",
    "\n",
    "# construct the train and test datasets\n",
    "data = NCF_Data(ml_100k)\n",
    "train_loader = data.get_train_instance()\n",
    "test_loader = data.get_test_instance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ---\n",
    "### TASK 1: Train the GMF and MLP separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model from Assignment 1:\n",
    "best = torch.load(\"models/NeuMF64_tuned.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmf_factors = best.gmf_user_embed[0].weight.shape[1] # num_factors for GMF model\n",
    "class GMF(nn.Module):\n",
    "    def __init__(self, num_users, num_items):\n",
    "        super(GMF, self).__init__()\n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "        self.num_factors = gmf_factors\n",
    "\n",
    "        self.embedding_user = nn.Embedding(\n",
    "            num_embeddings=self.num_users, embedding_dim=self.num_factors\n",
    "        )\n",
    "        self.embedding_item = nn.Embedding(\n",
    "            num_embeddings=self.num_items, embedding_dim=self.num_factors\n",
    "        )\n",
    "\n",
    "        self.affine_output = nn.Linear(in_features=self.num_factors, out_features=1)\n",
    "        self.logistic = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, user_indices, item_indices):\n",
    "        user_embedding = self.embedding_user(user_indices)\n",
    "        item_embedding = self.embedding_item(item_indices)\n",
    "        element_product = torch.mul(user_embedding, item_embedding)\n",
    "        logits = self.affine_output(element_product)\n",
    "        rating = self.logistic(logits)\n",
    "        return rating.squeeze()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and saving the GMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmf_model = GMF(num_users, num_items)\n",
    "train_model(gmf_model, train_loader, test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_factors = best.mlp_user_embed[0].weight.shape[1] # num_factors for GMF model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, num_users, num_items):\n",
    "        super(MLP, self).__init__()\n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "        self.num_factors = mlp_factors\n",
    "\n",
    "        self.embedding_user = nn.Embedding(\n",
    "            num_embeddings=num_users, embedding_dim=mlp_factors\n",
    "        )\n",
    "        self.embedding_item = nn.Embedding(\n",
    "            num_embeddings=num_items, embedding_dim=mlp_factors\n",
    "        )\n",
    "\n",
    "        layer_sizes = args[\"layers\"]\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(mlp_factors * 2, layer_sizes[0]))\n",
    "        layers.append(nn.ReLU())\n",
    "        for in_size, out_size in zip(layer_sizes[:-1], layer_sizes[1:]):\n",
    "            layers.append(nn.Linear(in_size, out_size))\n",
    "            layers.append(nn.ReLU())\n",
    "        self.mlp_fc = nn.Sequential(*layers)\n",
    "        self.mlp_fc.add_module(\"affine\", nn.Linear(layer_sizes[-1], 1))\n",
    "        self.mlp_fc.add_module(\"logit\", nn.Sigmoid())\n",
    "\n",
    "    def forward(self, user_indices, item_indices):\n",
    "        user_embedding = self.embedding_user(user_indices)\n",
    "        item_embedding = self.embedding_item(item_indices)\n",
    "        vector = torch.cat([user_embedding, item_embedding], dim=-1)\n",
    "        rating = self.mlp_fc(vector)\n",
    "        return rating.squeeze()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and saving the MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/giofratti/miniforge3/lib/python3.10/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/Users/giofratti/miniforge3/lib/python3.10/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "  File \"/Users/giofratti/Documents/GitHub/Neural-Collaborative-Filtering/utility.py\", line 7, in <module>\n",
      "    import pandas as pd\n",
      "  File \"/Users/giofratti/miniforge3/lib/python3.10/site-packages/pandas/__init__.py\", line 48, in <module>\n",
      "    from pandas.core.api import (\n",
      "  File \"/Users/giofratti/miniforge3/lib/python3.10/site-packages/pandas/core/api.py\", line 27, in <module>\n",
      "    from pandas.core.arrays import Categorical\n",
      "  File \"/Users/giofratti/miniforge3/lib/python3.10/site-packages/pandas/core/arrays/__init__.py\", line 8, in <module>\n",
      "    from pandas.core.arrays.categorical import Categorical\n",
      "  File \"/Users/giofratti/miniforge3/lib/python3.10/site-packages/pandas/core/arrays/categorical.py\", line 90, in <module>\n",
      "    from pandas.core.accessor import (\n",
      "  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 879, in exec_module\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 975, in get_code\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1074, in get_data\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [30], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m\n\u001b[1;32m      2\u001b[0m mlp_model \u001b[38;5;241m=\u001b[39m MLP(num_users, num_items)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmlp_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [28], line 17\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, test_loader, args)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Train step\u001b[39;00m\n\u001b[1;32m     15\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m user, item, label \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     18\u001b[0m     user \u001b[38;5;241m=\u001b[39m user\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     19\u001b[0m     item \u001b[38;5;241m=\u001b[39m item\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/torch/utils/data/dataloader.py:444\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterator\n\u001b[1;32m    443\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 444\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_iterator()\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/torch/utils/data/dataloader.py:390\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    389\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_worker_number_rationality()\n\u001b[0;32m--> 390\u001b[0m     \u001b[39mreturn\u001b[39;00m _MultiProcessingDataLoaderIter(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1077\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m   1070\u001b[0m w\u001b[39m.\u001b[39mdaemon \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   1071\u001b[0m \u001b[39m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[1;32m   1072\u001b[0m \u001b[39m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[1;32m   1073\u001b[0m \u001b[39m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[1;32m   1074\u001b[0m \u001b[39m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[1;32m   1075\u001b[0m \u001b[39m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[1;32m   1076\u001b[0m \u001b[39m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[0;32m-> 1077\u001b[0m w\u001b[39m.\u001b[39;49mstart()\n\u001b[1;32m   1078\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index_queues\u001b[39m.\u001b[39mappend(index_queue)\n\u001b[1;32m   1079\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_workers\u001b[39m.\u001b[39mappend(w)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/multiprocessing/process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m _current_process\u001b[39m.\u001b[39m_config\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mdaemon\u001b[39m\u001b[39m'\u001b[39m), \\\n\u001b[1;32m    119\u001b[0m        \u001b[39m'\u001b[39m\u001b[39mdaemonic processes are not allowed to have children\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    120\u001b[0m _cleanup()\n\u001b[0;32m--> 121\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_Popen(\u001b[39mself\u001b[39;49m)\n\u001b[1;32m    122\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sentinel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen\u001b[39m.\u001b[39msentinel\n\u001b[1;32m    123\u001b[0m \u001b[39m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[39m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/multiprocessing/context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m    223\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_Popen\u001b[39m(process_obj):\n\u001b[0;32m--> 224\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_context\u001b[39m.\u001b[39;49mget_context()\u001b[39m.\u001b[39;49mProcess\u001b[39m.\u001b[39;49m_Popen(process_obj)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/multiprocessing/context.py:284\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m    282\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_Popen\u001b[39m(process_obj):\n\u001b[1;32m    283\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpopen_spawn_posix\u001b[39;00m \u001b[39mimport\u001b[39;00m Popen\n\u001b[0;32m--> 284\u001b[0m     \u001b[39mreturn\u001b[39;00m Popen(process_obj)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/multiprocessing/popen_spawn_posix.py:32\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, process_obj):\n\u001b[1;32m     31\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fds \u001b[39m=\u001b[39m []\n\u001b[0;32m---> 32\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(process_obj)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/multiprocessing/popen_fork.py:19\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturncode \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinalizer \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_launch(process_obj)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/multiprocessing/popen_spawn_posix.py:62\u001b[0m, in \u001b[0;36mPopen._launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msentinel \u001b[39m=\u001b[39m parent_r\n\u001b[1;32m     61\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(parent_w, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m, closefd\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m---> 62\u001b[0m         f\u001b[39m.\u001b[39;49mwrite(fp\u001b[39m.\u001b[39;49mgetbuffer())\n\u001b[1;32m     63\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     fds_to_close \u001b[39m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "args['epochs'] = 20\n",
    "mlp_model = MLP(num_users, num_items)\n",
    "train_model(mlp_model, train_loader, test_loader, args) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NeuMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuMF_class(nn.Module):\n",
    "    def __init__(self, num_factors_gmf, num_factors_mlp, num_users, num_items):\n",
    "\n",
    "        super(NeuMF_class, self).__init__()\n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "        self.num_factors_gmf = gmf_factors\n",
    "        self.num_factors_mlp = mlp_factors\n",
    "        self.num_factors = max(num_factors_mlp, num_factors_gmf) #just for saving model name purpose\n",
    "\n",
    "        # GMF component\n",
    "        self.gmf_user_embed = nn.Embedding(num_embeddings=self.num_users, embedding_dim=self.num_factors_gmf)\n",
    "        self.gmf_item_embed = nn.Embedding(num_embeddings=self.num_items, embedding_dim=self.num_factors_gmf)\n",
    "        self.gmf_affine = nn.Linear(\n",
    "            in_features=self.num_factors_gmf, out_features=self.num_factors_gmf, bias=True\n",
    "        )\n",
    "\n",
    "        # MLP component\n",
    "        self.mlp_user_embed = nn.Embedding(num_embeddings=self.num_users, embedding_dim=self.num_factors_mlp)\n",
    "        self.mlp_item_embed = nn.Embedding(num_embeddings=self.num_items, embedding_dim=self.num_factors_mlp)\n",
    "\n",
    "        self.mlp_fc = nn.Sequential(\n",
    "            #We have considered as possible num_factors [8, 16, 32, 64] so this structure works\n",
    "            nn.Linear(2 * self.num_factors_mlp, self.num_factors_mlp),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(self.num_factors_mlp, int(self.num_factors_mlp / 2)),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(int(self.num_factors_mlp / 2), int(self.num_factors_mlp / 4)),  \n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Combine models\n",
    "        input_dim = self.num_factors_gmf + int(self.num_factors_mlp / 4)\n",
    "        self.mixing_layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, int(input_dim / 2)),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(int(input_dim / 2), int(input_dim / 4)),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(int(input_dim / 4), 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, user_indices, item_indices):\n",
    "        # GMF forward\n",
    "        user_embedding_gmf = self.gmf_user_embed(user_indices)\n",
    "        item_embedding_gmf = self.gmf_item_embed(item_indices)\n",
    "\n",
    "        element_product = torch.mul(user_embedding_gmf, item_embedding_gmf)\n",
    "        ratings_gmf = self.gmf_affine(element_product)\n",
    "\n",
    "        # MLP forward\n",
    "        user_embedding_mlp = self.mlp_user_embed(user_indices)\n",
    "        item_embedding_mlp = self.mlp_item_embed(item_indices)\n",
    "\n",
    "        vector = torch.cat((user_embedding_mlp, item_embedding_mlp), dim=-1)\n",
    "        ratings_mlp = self.mlp_fc(vector)\n",
    "\n",
    "        # Combine\n",
    "        ratings = torch.cat((ratings_gmf, ratings_mlp), dim=1)\n",
    "        return self.mixing_layers(ratings).squeeze()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuMF_f = NeuMF_class(gmf_factors, mlp_factors, num_users, num_items)\n",
    "neuMF_nof = NeuMF_class(gmf_factors, mlp_factors, num_users, num_items)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmf_model = torch.load(\"models_2/GMF8.pt\")\n",
    "mlp_model = torch.load(\"models_2/MLP64.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=32, out_features=16, bias=True)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the weights of the new model with the weights of the pretrained models\n",
    "# Freeze the weights of the pretrained models\n",
    "neuMF_f.gmf_user_embed.weight = gmf_model.embedding_user.weight\n",
    "neuMF_f.gmf_user_embed.requires_grad_(False)\n",
    "neuMF_f.gmf_item_embed.weight = gmf_model.embedding_item.weight\n",
    "neuMF_f.gmf_item_embed.requires_grad_(False)\n",
    "\n",
    "neuMF_f.mlp_user_embed.weight = mlp_model.embedding_user.weight\n",
    "neuMF_f.mlp_user_embed.requires_grad_(False)\n",
    "neuMF_f.mlp_item_embed.weight = mlp_model.embedding_item.weight\n",
    "neuMF_f.mlp_item_embed.requires_grad_(False)\n",
    "neuMF_f.mlp_fc[0].weight = mlp_model.mlp_fc[0].weight\n",
    "neuMF_f.mlp_fc[0].bias = mlp_model.mlp_fc[0].bias\n",
    "neuMF_f.mlp_fc[0].requires_grad_(False)\n",
    "neuMF_f.mlp_fc[2].weight = mlp_model.mlp_fc[2].weight\n",
    "neuMF_f.mlp_fc[2].bias = mlp_model.mlp_fc[2].bias\n",
    "neuMF_f.mlp_fc[2].requires_grad_(False)\n",
    "neuMF_f.mlp_fc[4].weight = mlp_model.mlp_fc[4].weight\n",
    "neuMF_f.mlp_fc[4].bias = mlp_model.mlp_fc[4].bias\n",
    "neuMF_f.mlp_fc[4].requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuMF_nof.gmf_user_embed.weight = gmf_model.embedding_user.weight\n",
    "neuMF_nof.gmf_item_embed.weight = gmf_model.embedding_item.weight\n",
    "\n",
    "neuMF_nof.mlp_user_embed.weight = mlp_model.embedding_user.weight\n",
    "neuMF_nof.mlp_item_embed.weight = mlp_model.embedding_item.weight\n",
    "neuMF_nof.mlp_fc[0].weight = mlp_model.mlp_fc[0].weight\n",
    "neuMF_nof.mlp_fc[0].bias = mlp_model.mlp_fc[0].bias\n",
    "neuMF_nof.mlp_fc[2].weight = mlp_model.mlp_fc[2].weight\n",
    "neuMF_nof.mlp_fc[2].bias = mlp_model.mlp_fc[2].bias\n",
    "neuMF_nof.mlp_fc[4].weight = mlp_model.mlp_fc[4].weight\n",
    "neuMF_nof.mlp_fc[4].bias = mlp_model.mlp_fc[4].bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000 | HR: 0.581 | time: 00: 00: 18\n",
      "Epoch 001 | HR: 0.587 | time: 00: 00: 18\n",
      "Epoch 002 | HR: 0.583 | time: 00: 00: 18\n",
      "Epoch 003 | HR: 0.582 | time: 00: 00: 18\n",
      "Epoch 004 | HR: 0.579 | time: 00: 00: 18\n",
      "Epoch 005 | HR: 0.583 | time: 00: 00: 18\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [31], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m15\u001b[39m\n\u001b[1;32m      2\u001b[0m args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.001\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneuMF_nof\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [28], line 17\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, test_loader, args)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Train step\u001b[39;00m\n\u001b[1;32m     15\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m user, item, label \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     18\u001b[0m     user \u001b[38;5;241m=\u001b[39m user\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     19\u001b[0m     item \u001b[38;5;241m=\u001b[39m item\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/torch/utils/data/dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    682\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    683\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    684\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    685\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1348\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1345\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1346\u001b[0m     \u001b[39m# no valid `self._rcvd_idx` is found (i.e., didn't break)\u001b[39;00m\n\u001b[1;32m   1347\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_persistent_workers:\n\u001b[0;32m-> 1348\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_shutdown_workers()\n\u001b[1;32m   1349\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m\n\u001b[1;32m   1351\u001b[0m \u001b[39m# Now `self._rcvd_idx` is the batch index we want to fetch\u001b[39;00m\n\u001b[1;32m   1352\u001b[0m \n\u001b[1;32m   1353\u001b[0m \u001b[39m# Check if the next sample has already been generated\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1474\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._shutdown_workers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1469\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mark_worker_as_unavailable(worker_id, shutdown\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m   1470\u001b[0m \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_workers:\n\u001b[1;32m   1471\u001b[0m     \u001b[39m# We should be able to join here, but in case anything went\u001b[39;00m\n\u001b[1;32m   1472\u001b[0m     \u001b[39m# wrong, we set a timeout and if the workers fail to join,\u001b[39;00m\n\u001b[1;32m   1473\u001b[0m     \u001b[39m# they are killed in the `finally` block.\u001b[39;00m\n\u001b[0;32m-> 1474\u001b[0m     w\u001b[39m.\u001b[39;49mjoin(timeout\u001b[39m=\u001b[39;49m_utils\u001b[39m.\u001b[39;49mMP_STATUS_CHECK_INTERVAL)\n\u001b[1;32m   1475\u001b[0m \u001b[39mfor\u001b[39;00m q \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index_queues:\n\u001b[1;32m   1476\u001b[0m     q\u001b[39m.\u001b[39mcancel_join_thread()\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/multiprocessing/process.py:149\u001b[0m, in \u001b[0;36mBaseProcess.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_parent_pid \u001b[39m==\u001b[39m os\u001b[39m.\u001b[39mgetpid(), \u001b[39m'\u001b[39m\u001b[39mcan only join a child process\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    148\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39m'\u001b[39m\u001b[39mcan only join a started process\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 149\u001b[0m res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_popen\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    150\u001b[0m \u001b[39mif\u001b[39;00m res \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m     _children\u001b[39m.\u001b[39mdiscard(\u001b[39mself\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/multiprocessing/popen_fork.py:40\u001b[0m, in \u001b[0;36mPopen.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mmultiprocessing\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconnection\u001b[39;00m \u001b[39mimport\u001b[39;00m wait\n\u001b[0;32m---> 40\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m wait([\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msentinel], timeout):\n\u001b[1;32m     41\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[39m# This shouldn't block if wait() returned successfully.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/multiprocessing/connection.py:936\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    933\u001b[0m     deadline \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mmonotonic() \u001b[39m+\u001b[39m timeout\n\u001b[1;32m    935\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 936\u001b[0m     ready \u001b[39m=\u001b[39m selector\u001b[39m.\u001b[39;49mselect(timeout)\n\u001b[1;32m    937\u001b[0m     \u001b[39mif\u001b[39;00m ready:\n\u001b[1;32m    938\u001b[0m         \u001b[39mreturn\u001b[39;00m [key\u001b[39m.\u001b[39mfileobj \u001b[39mfor\u001b[39;00m (key, events) \u001b[39min\u001b[39;00m ready]\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/selectors.py:416\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m ready \u001b[39m=\u001b[39m []\n\u001b[1;32m    415\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     fd_event_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_selector\u001b[39m.\u001b[39;49mpoll(timeout)\n\u001b[1;32m    417\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mInterruptedError\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m     \u001b[39mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "args['epochs'] = 15\n",
    "args['lr'] = 0.001\n",
    "train_model(neuMF_nof, train_loader, test_loader, args) # Freezed initialized layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuMF_class(\n",
       "  (gmf_user_embed): Embedding(944, 8)\n",
       "  (gmf_item_embed): Embedding(1683, 8)\n",
       "  (gmf_affine): Linear(in_features=8, out_features=8, bias=True)\n",
       "  (mlp_user_embed): Embedding(944, 64)\n",
       "  (mlp_item_embed): Embedding(1683, 64)\n",
       "  (mlp_fc): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=32, out_features=16, bias=True)\n",
       "    (5): ReLU()\n",
       "  )\n",
       "  (mixing_layers): Sequential(\n",
       "    (0): Linear(in_features=24, out_features=12, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=12, out_features=6, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=6, out_features=1, bias=True)\n",
       "    (5): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neuMF_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (embedding_user): Embedding(944, 64)\n",
       "  (embedding_item): Embedding(1683, 64)\n",
       "  (mlp_fc): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=32, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=32, out_features=16, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=16, out_features=8, bias=True)\n",
       "    (5): ReLU()\n",
       "    (affine): Linear(in_features=8, out_features=1, bias=True)\n",
       "    (logit): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit ('miniforge3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d29b108309575d5ffeaef586407ef10816dbaacc527dc95f14090a22594fdff0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
